{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":3053020,"sourceType":"datasetVersion","datasetId":1869236},{"sourceId":4368115,"sourceType":"datasetVersion","datasetId":1152384}],"dockerImageVersionId":30746,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"This Jupyter notebook aims to classify bullying tweets using a variety of traditional machine learning and deep learning models, and then compares their performances with each other.","metadata":{}},{"cell_type":"markdown","source":"# Table of Contents","metadata":{}},{"cell_type":"markdown","source":"- [1 - Data Import & Data Cleaning](#1)\n    - [1.1 - Installing & Import libraries](#1.1)\n    - [1.1 - Importing Datasets](#1.2)\n    - [1.2 - Cleaning](#1.3)\n- [2 - Analyzing the dataset](#2)\n    - [2.1 - Category-wise Analysis](#2.1)\n    - [2.2 - Tweets length Analysis](#2.2)\n- [3 - Traditional Models](#3)\n    - [3.1 - Random Forest](#3.1)\n    - [3.2 - Gradient Boosting](#3.2)\n    - [3.3 - Naive Bayes](#3.3)\n    - [3.4 - Logistic Regression](#3.4)\n    - [3.5 - SVC](#3.5)\n    - [3.6 - Stacking](#3.6)\n    - [3.7 - Comparing the evaluations of traditional models](#3.7)\n- [4 - Deep Learning Models](#4)\n    - [4.1 - Simple LSTM](#4.1)\n    - [4.2 - Fine Tuned LSTM](#4.2)\n    - [4.3 - GRU](#4.3)\n    - [4.4 - GRU (GloVe Embedding)](#4.4)\n    - [4.5 - GRU (Word2Vec & Attention)](#4.5)\n    - [4.6 - Bert](#4.6)\n- [5 - Evaluation of all Models](#5)","metadata":{}},{"cell_type":"markdown","source":"<a name='1'></a>\n\n# Data Import & Data Cleaning","metadata":{}},{"cell_type":"markdown","source":"<a name='1.1'></a>\n## Installing & Import libraries","metadata":{}},{"cell_type":"code","source":"! pip install langdetect\n! pip install contractions\n! pip install emoji\n! pip install imblearn\n! pip install torch\n! pip install transformers\n! pip install demoji\n! pip install nltk\n! pip install gensim\n! pip install plotly","metadata":{"execution":{"iopub.status.busy":"2024-10-24T17:03:34.989136Z","iopub.execute_input":"2024-10-24T17:03:34.990234Z","iopub.status.idle":"2024-10-24T17:05:43.258017Z","shell.execute_reply.started":"2024-10-24T17:03:34.990195Z","shell.execute_reply":"2024-10-24T17:05:43.256582Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!pip install keras","metadata":{"execution":{"iopub.status.busy":"2024-10-24T17:05:43.260688Z","iopub.execute_input":"2024-10-24T17:05:43.261182Z","iopub.status.idle":"2024-10-24T17:05:55.541203Z","shell.execute_reply.started":"2024-10-24T17:05:43.261136Z","shell.execute_reply":"2024-10-24T17:05:55.539609Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# General purpose\nimport pandas as pd\nimport numpy as np\nimport time\nimport warnings\nwarnings.filterwarnings(\"ignore\")\nimport os\n\n# Text cleaning\nimport re\nimport string\nimport emoji\nimport demoji\nimport nltk\nfrom nltk.stem import WordNetLemmatizer, PorterStemmer\nfrom nltk.corpus import stopwords\n# Stop words for text cleaning\nnltk.download('stopwords')\nstop_words = set(stopwords.words('english'))\nnltk.download('punkt')  # Download the punkt tokenizer\nnltk.download('wordnet')  # Download WordNet for lemmatization\n\n# Data preprocessing\nfrom sklearn import preprocessing\nfrom sklearn.model_selection import StratifiedShuffleSplit, train_test_split\nfrom langdetect import detect, LangDetectException\nfrom sklearn.preprocessing import OneHotEncoder\nimport contractions\nfrom nltk.tokenize import word_tokenize\n\n# Balancing\nfrom imblearn.combine import SMOTEENN\nfrom imblearn.over_sampling import RandomOverSampler\n\n# Evaluation metrics\nfrom sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, classification_report\nfrom sklearn.metrics import precision_recall_curve, roc_auc_score, average_precision_score, confusion_matrix, roc_curve, auc\n\n# Traditional Machine Learning Models\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.feature_extraction.text import TfidfTransformer\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, StackingClassifier\nfrom sklearn.svm import SVC\n\n# PyTorch for Deep Learning models\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\nfrom torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n\n# Tensorflow for Deep Learning models\nimport tensorflow as tf\nfrom tensorflow.keras.preprocessing.text import Tokenizer\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Embedding, LSTM, Dense, GRU, Dropout, SimpleRNN\nfrom tensorflow.keras.callbacks import EarlyStopping\nfrom tensorflow.keras.layers import BatchNormalization\n\n# Tokenization for GRU (word2Vec Embedding)\nfrom collections import Counter\nfrom gensim.models import Word2Vec\n\n# Transformers for BERT\nimport transformers\nfrom transformers import BertModel\nfrom transformers import BertTokenizer\nfrom transformers import AdamW, get_linear_schedule_with_warmup\n\n# Set seed for reproducibility\nimport random\nseed_value = 2042\nrandom.seed(seed_value)\nnp.random.seed(seed_value)\ntorch.manual_seed(seed_value)\ntorch.cuda.manual_seed_all(seed_value)\n\n# Visualization\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nsns.despine()\nplt.rc(\"figure\", autolayout=True)\nplt.rc(\"axes\", labelweight=\"bold\", labelsize=\"large\", titleweight=\"bold\", titlepad=10)\nfrom plotly.offline import iplot\nimport plotly.graph_objs as go\nfrom plotly.subplots import make_subplots\nfrom wordcloud import WordCloud\n","metadata":{"execution":{"iopub.status.busy":"2024-10-24T17:54:45.865435Z","iopub.execute_input":"2024-10-24T17:54:45.865847Z","iopub.status.idle":"2024-10-24T17:54:45.946559Z","shell.execute_reply.started":"2024-10-24T17:54:45.865816Z","shell.execute_reply":"2024-10-24T17:54:45.945447Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import nltk\n\n# Specify the directory to download NLTK data\nnltk_data_dir = \"/kaggle/working/nltk_data\"\n\n# Download the necessary NLTK resources to the specified directory\nnltk.download('wordnet', download_dir=nltk_data_dir)\nnltk.download('omw-1.4', download_dir=nltk_data_dir)\nnltk.download('punkt', download_dir=nltk_data_dir)\n\n# Set the NLTK data path\nnltk.data.path.append(nltk_data_dir)\n\nfrom nltk.stem import WordNetLemmatizer\nfrom nltk.tokenize import word_tokenize","metadata":{"execution":{"iopub.status.busy":"2024-10-24T17:54:51.557068Z","iopub.execute_input":"2024-10-24T17:54:51.557498Z","iopub.status.idle":"2024-10-24T17:54:51.682704Z","shell.execute_reply.started":"2024-10-24T17:54:51.557463Z","shell.execute_reply":"2024-10-24T17:54:51.68142Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\nimport zipfile\n\n# Define the paths for the zip files\nnltk_data_dir = \"/kaggle/working/nltk_data\"\ncorpora_dir = os.path.join(nltk_data_dir, \"corpora\")\n\n# Ensure the corpora directory exists\nif not os.path.exists(corpora_dir):\n    os.makedirs(corpora_dir)\n\n# Unzip the wordnet.zip and omw-1.4.zip files into the corpora directory\nfor zip_file in [\"wordnet.zip\", \"omw-1.4.zip\"]:\n    zip_path = os.path.join(corpora_dir, zip_file)\n    with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n        zip_ref.extractall(corpora_dir)\n\n# Check if the data has been unzipped correctly\nprint(\"Contents of corpora directory after unzipping:\", os.listdir(corpora_dir))\n\n# Verify that NLTK can find the WordNet corpus\nimport nltk\nnltk.data.path.append(nltk_data_dir)\n\ntry:\n    from nltk.corpus import wordnet\n    # Check if we can access the WordNet corpus\n    print(\"WordNet path:\", wordnet.root)\n    print(\"Sample Synset:\", wordnet.synsets('computer'))\nexcept LookupError as e:\n    print(\"Error accessing WordNet:\", e)","metadata":{"execution":{"iopub.status.busy":"2024-10-24T17:54:56.592546Z","iopub.execute_input":"2024-10-24T17:54:56.592956Z","iopub.status.idle":"2024-10-24T17:54:57.555769Z","shell.execute_reply.started":"2024-10-24T17:54:56.592917Z","shell.execute_reply":"2024-10-24T17:54:57.554277Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"<a name='1.2'></a>\n## Importing Datasets","metadata":{}},{"cell_type":"markdown","source":"### First (Main) Dataset\nIn this study, the Fine-Grained Cyberbullying Dataset (FGCD) was used [kaggle](https://www.kaggle.com/datasets/andrewmvd/cyberbullying-classification), consisting of 47,692 labelled tweets distributed across distinct categories: Age, Ethnicity, Gender, Religion, Other-cyberbullying, and Not-cyberbullying. the dataset consists of 47,692 labelled tweets, with approximately 8000 rows for each class. Thus, it appears to be a balanced dataset","metadata":{}},{"cell_type":"code","source":"import warnings\n\nwarnings.filterwarnings(\"ignore\", category=FutureWarning)","metadata":{"execution":{"iopub.status.busy":"2024-10-24T17:55:06.144502Z","iopub.execute_input":"2024-10-24T17:55:06.144957Z","iopub.status.idle":"2024-10-24T17:55:06.151194Z","shell.execute_reply.started":"2024-10-24T17:55:06.144921Z","shell.execute_reply":"2024-10-24T17:55:06.149804Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"input_dir = '/kaggle/input'\n\n# Find and read the CSV file\nfor dirname, _, filenames in os.walk(input_dir):\n    for filename in filenames:\n        file_path = os.path.join(dirname, filename)\n        print(file_path)  \n\n        # Check if the file is the one you need\n        if filename == 'cyberbullying_tweets.csv':\n            # Read the CSV file into a DataFrame\n            df = pd.read_csv(file_path)\n            print(\"File loaded successfully!\")","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-10-24T17:55:07.508316Z","iopub.execute_input":"2024-10-24T17:55:07.508713Z","iopub.status.idle":"2024-10-24T17:55:07.667682Z","shell.execute_reply.started":"2024-10-24T17:55:07.508685Z","shell.execute_reply":"2024-10-24T17:55:07.666186Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df","metadata":{"execution":{"iopub.status.busy":"2024-10-24T17:55:09.339452Z","iopub.execute_input":"2024-10-24T17:55:09.339852Z","iopub.status.idle":"2024-10-24T17:55:09.352908Z","shell.execute_reply.started":"2024-10-24T17:55:09.339822Z","shell.execute_reply":"2024-10-24T17:55:09.351637Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Second Dataset\nAfter data cleaning, significant class distribution imbalances were detected, particularly in the 'Other-cyberbullying' and 'Not-cyberbullying' classes. To address this, the RandomOverSampler was initially implemented. However, after testing different classification models, the accuracy and F1 scores for these classes remained notably lower. Therefore, a supplementary dataset with binary labels was introduced in this stage [kaggle](https://www.kaggle.com/datasets/saurabhshahane/cyberbullying-dataset). 1700 instances from the ‘non-bullying’ class were integrated to augment the 'Not-cyberbullying' class. Due to the diverse nature of the ‘bully’ class in the second dataset, filling the ‘Other cyberbullying’ class using this dataset seems illogical, leading to the removal of this column from the main dataset to maintain consistency.","metadata":{}},{"cell_type":"code","source":"input_dir = '/kaggle/input'\n\n# Find and read the CSV file\nfor dirname, _, filenames in os.walk(input_dir):\n    for filename in filenames:\n        file_path = os.path.join(dirname, filename)\n        print(file_path)  \n\n        # Check if the file is the one you need\n        if filename == 'aggression_parsed_dataset.csv':\n            # Read the CSV file into a DataFrame\n            df2 = pd.read_csv(file_path)\n            print(\"File loaded successfully!\")","metadata":{"execution":{"iopub.status.busy":"2024-10-24T17:55:12.097509Z","iopub.execute_input":"2024-10-24T17:55:12.097921Z","iopub.status.idle":"2024-10-24T17:55:12.885091Z","shell.execute_reply.started":"2024-10-24T17:55:12.097891Z","shell.execute_reply":"2024-10-24T17:55:12.883843Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df2","metadata":{"execution":{"iopub.status.busy":"2024-10-24T17:55:14.467641Z","iopub.execute_input":"2024-10-24T17:55:14.468065Z","iopub.status.idle":"2024-10-24T17:55:14.483984Z","shell.execute_reply.started":"2024-10-24T17:55:14.468033Z","shell.execute_reply":"2024-10-24T17:55:14.482801Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"useful_columns = ['Text', 'oh_label']\ndf2 = df2[useful_columns]","metadata":{"execution":{"iopub.status.busy":"2024-10-24T17:55:14.524664Z","iopub.execute_input":"2024-10-24T17:55:14.525086Z","iopub.status.idle":"2024-10-24T17:55:14.536853Z","shell.execute_reply.started":"2024-10-24T17:55:14.525055Z","shell.execute_reply":"2024-10-24T17:55:14.535366Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df2['oh_label'].value_counts()","metadata":{"execution":{"iopub.status.busy":"2024-10-24T17:55:17.179368Z","iopub.execute_input":"2024-10-24T17:55:17.179746Z","iopub.status.idle":"2024-10-24T17:55:17.190276Z","shell.execute_reply.started":"2024-10-24T17:55:17.179718Z","shell.execute_reply":"2024-10-24T17:55:17.189045Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df2 = df2.rename(columns={'Text':'tweet_text','oh_label': 'cyberbullying_type'})","metadata":{"execution":{"iopub.status.busy":"2024-10-24T17:55:17.532818Z","iopub.execute_input":"2024-10-24T17:55:17.533265Z","iopub.status.idle":"2024-10-24T17:55:17.545144Z","shell.execute_reply.started":"2024-10-24T17:55:17.53321Z","shell.execute_reply":"2024-10-24T17:55:17.543486Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df2","metadata":{"execution":{"iopub.status.busy":"2024-10-24T17:55:18.58777Z","iopub.execute_input":"2024-10-24T17:55:18.588263Z","iopub.status.idle":"2024-10-24T17:55:18.600941Z","shell.execute_reply.started":"2024-10-24T17:55:18.588216Z","shell.execute_reply":"2024-10-24T17:55:18.599692Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df2.iloc[115840][0]","metadata":{"execution":{"iopub.status.busy":"2024-10-24T17:55:19.205871Z","iopub.execute_input":"2024-10-24T17:55:19.206284Z","iopub.status.idle":"2024-10-24T17:55:19.215657Z","shell.execute_reply.started":"2024-10-24T17:55:19.206237Z","shell.execute_reply":"2024-10-24T17:55:19.214197Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Tweets belonging to the category \"not bully\" is selected.\n\nnot_bully_df = df2[df2['cyberbullying_type'] == 0]","metadata":{"execution":{"iopub.status.busy":"2024-10-24T17:55:19.682207Z","iopub.execute_input":"2024-10-24T17:55:19.682615Z","iopub.status.idle":"2024-10-24T17:55:19.703553Z","shell.execute_reply.started":"2024-10-24T17:55:19.682587Z","shell.execute_reply":"2024-10-24T17:55:19.70229Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# the length of tweets is measured\nnot_bully_df['text_len'] = [len(text.split()) for text in not_bully_df.tweet_text];\nnot_bully_df","metadata":{"execution":{"iopub.status.busy":"2024-10-24T17:55:20.401697Z","iopub.execute_input":"2024-10-24T17:55:20.402114Z","iopub.status.idle":"2024-10-24T17:55:20.929102Z","shell.execute_reply.started":"2024-10-24T17:55:20.40208Z","shell.execute_reply":"2024-10-24T17:55:20.927974Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Because it has been understood in the next stages that 99% of the tweets in the main dataset are below 32 words, it has been decided to only include tweets with a length below 50 words randomly in the main dataset.\n\nIt is noted that the size of tweets will be reduced after cleaning and removing certain characters or stop words.","metadata":{}},{"cell_type":"code","source":"not_bully_df = not_bully_df[not_bully_df['text_len']<50] # tweets which their lenth are below 50\nrandom_rows = not_bully_df.sample(n=1700) # 1700 tweets are added\nrandom_rows","metadata":{"execution":{"iopub.status.busy":"2024-10-24T17:55:25.49903Z","iopub.execute_input":"2024-10-24T17:55:25.499442Z","iopub.status.idle":"2024-10-24T17:55:25.522473Z","shell.execute_reply.started":"2024-10-24T17:55:25.499411Z","shell.execute_reply":"2024-10-24T17:55:25.521313Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":" # changing the labels from 0 to not_cyberbulling\nrandom_rows['cyberbullying_type'] = random_rows['cyberbullying_type'].replace(0, 'not_cyberbullying')\n# dropping the tweet lenth after filtering\nrandom_rows = random_rows.drop(columns=['text_len'])\n","metadata":{"execution":{"iopub.status.busy":"2024-10-24T17:55:29.936048Z","iopub.execute_input":"2024-10-24T17:55:29.936477Z","iopub.status.idle":"2024-10-24T17:55:29.944421Z","shell.execute_reply.started":"2024-10-24T17:55:29.936445Z","shell.execute_reply":"2024-10-24T17:55:29.94308Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df = pd.concat([df, random_rows])\ndf","metadata":{"execution":{"iopub.status.busy":"2024-10-24T17:55:30.592437Z","iopub.execute_input":"2024-10-24T17:55:30.592837Z","iopub.status.idle":"2024-10-24T17:55:30.609896Z","shell.execute_reply.started":"2024-10-24T17:55:30.592808Z","shell.execute_reply":"2024-10-24T17:55:30.608307Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df.info()","metadata":{"execution":{"iopub.status.busy":"2024-10-24T17:55:31.191552Z","iopub.execute_input":"2024-10-24T17:55:31.191926Z","iopub.status.idle":"2024-10-24T17:55:31.213026Z","shell.execute_reply.started":"2024-10-24T17:55:31.1919Z","shell.execute_reply":"2024-10-24T17:55:31.211771Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Renaming the columns name\ndf = df.rename(columns={'tweet_text': 'text', 'cyberbullying_type': 'sentiment'})","metadata":{"execution":{"iopub.status.busy":"2024-10-24T17:55:31.673437Z","iopub.execute_input":"2024-10-24T17:55:31.674646Z","iopub.status.idle":"2024-10-24T17:55:31.689627Z","shell.execute_reply.started":"2024-10-24T17:55:31.674596Z","shell.execute_reply":"2024-10-24T17:55:31.688164Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Finding duplicated tweets and dropping them\nprint(f'Number of duplicated tweets',df.duplicated().sum())\ndf = df[~df.duplicated()]","metadata":{"execution":{"iopub.status.busy":"2024-10-24T17:55:32.105675Z","iopub.execute_input":"2024-10-24T17:55:32.106103Z","iopub.status.idle":"2024-10-24T17:55:32.193952Z","shell.execute_reply.started":"2024-10-24T17:55:32.10607Z","shell.execute_reply":"2024-10-24T17:55:32.192743Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df['sentiment'].value_counts()","metadata":{"execution":{"iopub.status.busy":"2024-10-24T17:55:32.535772Z","iopub.execute_input":"2024-10-24T17:55:32.536795Z","iopub.status.idle":"2024-10-24T17:55:32.553144Z","shell.execute_reply.started":"2024-10-24T17:55:32.536754Z","shell.execute_reply":"2024-10-24T17:55:32.551596Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"At this stage, it is observed that the number of instances belonging to the not_cyberbullying class is significantly higher than the other classes. However, after cleaning, its count will decrease, and it can then be considered in a balanced situation along with the other classes.","metadata":{}},{"cell_type":"markdown","source":"<a name='1.3'></a>\n\n## Data Cleaning","metadata":{}},{"cell_type":"markdown","source":"### Define Cleaning Functions","metadata":{}},{"cell_type":"code","source":"# Clean emojis from text\ndef remove_emoji(text):\n    return demoji.replace(text, '')\n\n#demoji.replace_with_desc()","metadata":{"execution":{"iopub.status.busy":"2024-10-24T17:55:34.229811Z","iopub.execute_input":"2024-10-24T17:55:34.230214Z","iopub.status.idle":"2024-10-24T17:55:34.236045Z","shell.execute_reply.started":"2024-10-24T17:55:34.230182Z","shell.execute_reply":"2024-10-24T17:55:34.234867Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Remove punctuations, stopwords, links, mentions and new line characters\ndef remove_all_entities(text):\n    # Replacing newline and carriage return characters with a space and converting the text to lowercase\n    text = re.sub(r'\\r|\\n', ' ', text.lower())  \n    # Removing links and mentions\n    text = re.sub(r\"(?:\\@|https?\\://)\\S+\", \"\", text)  \n    # Removing non-ASCII characters\n    text = re.sub(r'[^\\x00-\\x7f]', '', text)\n    \n    # Defining the list of punctuation characters\n    banned_list = string.punctuation\n    # Creating a translation table to remove punctuation characters from text\n    table = str.maketrans('', '', banned_list)\n    # Removing punctuation characters from the text using the translation table\n    text = text.translate(table)\n    # Tokenizing the text into individual words, excluding stop words, and join them back into a single string\n    text = ' '.join(word for word in text.split() if word not in stop_words)\n    return text","metadata":{"execution":{"iopub.status.busy":"2024-10-24T17:55:34.578693Z","iopub.execute_input":"2024-10-24T17:55:34.579125Z","iopub.status.idle":"2024-10-24T17:55:34.587624Z","shell.execute_reply.started":"2024-10-24T17:55:34.579092Z","shell.execute_reply":"2024-10-24T17:55:34.586263Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Remove '#' symbols from hashtags at the end of the sentence, and keep those in the middle of the sentence\ndef clean_hashtags(tweet):\n    # Removing hashtags at the end of the sentence\n    new_tweet = re.sub(r'(\\s+#[\\w-]+)+\\s*$', '', tweet).strip()\n    \n    # Removing the # symbol from hashtags in the middle of the sentence\n    new_tweet = re.sub(r'#([\\w-]+)', r'\\1', new_tweet).strip()\n    \n    return new_tweet","metadata":{"execution":{"iopub.status.busy":"2024-10-24T17:55:34.918206Z","iopub.execute_input":"2024-10-24T17:55:34.918634Z","iopub.status.idle":"2024-10-24T17:55:34.925237Z","shell.execute_reply.started":"2024-10-24T17:55:34.918602Z","shell.execute_reply":"2024-10-24T17:55:34.923587Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Remove special characters such as & and $ present in some words\ndef remove_chars(text):\n    return ' '.join('' if ('$' in word) or ('&' in word) else word for word in text.split())","metadata":{"execution":{"iopub.status.busy":"2024-10-24T17:55:35.232723Z","iopub.execute_input":"2024-10-24T17:55:35.23317Z","iopub.status.idle":"2024-10-24T17:55:35.239666Z","shell.execute_reply.started":"2024-10-24T17:55:35.233136Z","shell.execute_reply":"2024-10-24T17:55:35.238162Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Remove multiple spaces\ndef remove_mult_spaces(text):\n    return re.sub(r\"\\s\\s+\", \" \", text)","metadata":{"execution":{"iopub.status.busy":"2024-10-24T17:55:35.577497Z","iopub.execute_input":"2024-10-24T17:55:35.577931Z","iopub.status.idle":"2024-10-24T17:55:35.584071Z","shell.execute_reply.started":"2024-10-24T17:55:35.577897Z","shell.execute_reply":"2024-10-24T17:55:35.582733Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Check if the text is in English, and return an empty string if it's not\ndef remove_non_english(text):\n    try:\n        language = detect(text)\n    except LangDetectException:\n        language = \"unknown\"\n    return text if language == \"en\" else \"\"","metadata":{"execution":{"iopub.status.busy":"2024-10-24T17:55:35.922267Z","iopub.execute_input":"2024-10-24T17:55:35.922645Z","iopub.status.idle":"2024-10-24T17:55:35.928699Z","shell.execute_reply.started":"2024-10-24T17:55:35.922618Z","shell.execute_reply":"2024-10-24T17:55:35.927313Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Expand contractions\ndef expand_contractions(text):\n    return contractions.fix(text)","metadata":{"execution":{"iopub.status.busy":"2024-10-24T17:55:36.245239Z","iopub.execute_input":"2024-10-24T17:55:36.245683Z","iopub.status.idle":"2024-10-24T17:55:36.25148Z","shell.execute_reply.started":"2024-10-24T17:55:36.24565Z","shell.execute_reply":"2024-10-24T17:55:36.25001Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Remove numbers\ndef remove_numbers(text):\n    return re.sub(r'\\d+', '', text)","metadata":{"execution":{"iopub.status.busy":"2024-10-24T17:55:36.609803Z","iopub.execute_input":"2024-10-24T17:55:36.61023Z","iopub.status.idle":"2024-10-24T17:55:36.61573Z","shell.execute_reply.started":"2024-10-24T17:55:36.610199Z","shell.execute_reply":"2024-10-24T17:55:36.614558Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Initialize lemmatizer for text cleaning\nlemmatizer = WordNetLemmatizer()\n# Lemmatize words\ndef lemmatize(text):\n    # Tokenizing the input text into individual words\n    words = word_tokenize(text)\n    # Lemmatizing each word in the tokenized text using the lemmatizer\n    lemmatized_words = [lemmatizer.lemmatize(word) for word in words]\n    # Joining the lemmatized words back into a single string\n    return ' '.join(lemmatized_words)","metadata":{"execution":{"iopub.status.busy":"2024-10-24T17:55:37.005407Z","iopub.execute_input":"2024-10-24T17:55:37.005841Z","iopub.status.idle":"2024-10-24T17:55:37.012273Z","shell.execute_reply.started":"2024-10-24T17:55:37.005806Z","shell.execute_reply":"2024-10-24T17:55:37.01065Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Remove short words\ndef remove_short_words(text, min_len=2):\n    # Spliting the input text into individual words\n    words = text.split()\n    # Filtering out words shorter than the specified minimum length\n    long_words = [word for word in words if len(word) >= min_len]\n    # Joining the remaining long words back into a single string\n    return ' '.join(long_words)","metadata":{"execution":{"iopub.status.busy":"2024-10-24T17:55:37.36286Z","iopub.execute_input":"2024-10-24T17:55:37.363334Z","iopub.status.idle":"2024-10-24T17:55:37.369886Z","shell.execute_reply.started":"2024-10-24T17:55:37.363298Z","shell.execute_reply":"2024-10-24T17:55:37.368696Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Replace elongated words with their base form\n#Elongated words are words that contain repeated characters, such as \"loooove\" for \"love\" or \"cooool\" for \"cool\".\ndef correct_elongated_words(text):\n    # Defining a regular expression pattern to match elongated words\n    regular_pattern = r'\\b(\\w+)((\\w)\\3{2,})(\\w*)\\b'\n    # Using the regular expression substitution to correct elongated words\n    # Replacing the elongated part of the word with a single occurrence of the repeated character\n    return re.sub(regular_pattern, r'\\1\\3\\4', text)","metadata":{"execution":{"iopub.status.busy":"2024-10-24T17:55:37.803551Z","iopub.execute_input":"2024-10-24T17:55:37.804004Z","iopub.status.idle":"2024-10-24T17:55:37.810166Z","shell.execute_reply.started":"2024-10-24T17:55:37.80397Z","shell.execute_reply":"2024-10-24T17:55:37.808724Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Remove repeated punctuation\ndef remove_repeated_punctuation(text):\n    return re.sub(r'[\\?\\.\\!]+(?=[\\?\\.\\!])', '', text)","metadata":{"execution":{"iopub.status.busy":"2024-10-24T17:55:38.12769Z","iopub.execute_input":"2024-10-24T17:55:38.128097Z","iopub.status.idle":"2024-10-24T17:55:38.134652Z","shell.execute_reply.started":"2024-10-24T17:55:38.128065Z","shell.execute_reply":"2024-10-24T17:55:38.133221Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Remove extra whitespace\ndef remove_extra_whitespace(text):\n    return ' '.join(text.split())","metadata":{"execution":{"iopub.status.busy":"2024-10-24T17:55:38.551518Z","iopub.execute_input":"2024-10-24T17:55:38.552425Z","iopub.status.idle":"2024-10-24T17:55:38.558637Z","shell.execute_reply.started":"2024-10-24T17:55:38.552385Z","shell.execute_reply":"2024-10-24T17:55:38.557141Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def remove_url_shorteners(text):\n    return re.sub(r'(?:http[s]?://)?(?:www\\.)?(?:bit\\.ly|goo\\.gl|t\\.co|tinyurl\\.com|tr\\.im|is\\.gd|cli\\.gs|u\\.nu|url\\.ie|tiny\\.cc|alturl\\.com|ow\\.ly|bit\\.do|adoro\\.to)\\S+', '', text)","metadata":{"execution":{"iopub.status.busy":"2024-10-24T17:55:39.016806Z","iopub.execute_input":"2024-10-24T17:55:39.017234Z","iopub.status.idle":"2024-10-24T17:55:39.023453Z","shell.execute_reply.started":"2024-10-24T17:55:39.017201Z","shell.execute_reply":"2024-10-24T17:55:39.02161Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Remove spaces at the beginning and end of the tweet\ndef remove_spaces_tweets(tweet):\n    return tweet.strip()","metadata":{"execution":{"iopub.status.busy":"2024-10-24T17:55:39.330671Z","iopub.execute_input":"2024-10-24T17:55:39.331105Z","iopub.status.idle":"2024-10-24T17:55:39.336397Z","shell.execute_reply.started":"2024-10-24T17:55:39.331072Z","shell.execute_reply":"2024-10-24T17:55:39.33512Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Remove short tweets\ndef remove_short_tweets(tweet, min_words=3):\n    words = tweet.split()\n    return tweet if len(words) >= min_words else \"\"","metadata":{"execution":{"iopub.status.busy":"2024-10-24T17:55:39.804451Z","iopub.execute_input":"2024-10-24T17:55:39.804922Z","iopub.status.idle":"2024-10-24T17:55:39.811277Z","shell.execute_reply.started":"2024-10-24T17:55:39.804888Z","shell.execute_reply":"2024-10-24T17:55:39.809923Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Function to call all the cleaning functions in the correct order\ndef clean_tweet(tweet):\n    tweet = remove_emoji(tweet)\n    tweet = expand_contractions(tweet)\n    tweet = remove_non_english(tweet)\n    tweet = remove_all_entities(tweet)\n    tweet = clean_hashtags(tweet)\n    tweet = remove_chars(tweet)\n    tweet = remove_mult_spaces(tweet)\n    tweet = remove_numbers(tweet)\n    tweet = lemmatize(tweet)\n    tweet = remove_short_words(tweet)\n    tweet = correct_elongated_words(tweet)\n    tweet = remove_repeated_punctuation(tweet)\n    tweet = remove_extra_whitespace(tweet)\n    tweet = remove_url_shorteners(tweet)\n    tweet = remove_spaces_tweets(tweet)\n    tweet = remove_short_tweets(tweet)\n    tweet = ' '.join(tweet.split())  # Remove multiple spaces between words\n    return tweet","metadata":{"execution":{"iopub.status.busy":"2024-10-24T17:55:40.21168Z","iopub.execute_input":"2024-10-24T17:55:40.212108Z","iopub.status.idle":"2024-10-24T17:55:40.219726Z","shell.execute_reply.started":"2024-10-24T17:55:40.212077Z","shell.execute_reply":"2024-10-24T17:55:40.21846Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Applying the cleaning functions on dataset","metadata":{}},{"cell_type":"code","source":"df['text_clean'] = [clean_tweet(tweet) for tweet in df['text']]","metadata":{"execution":{"iopub.status.busy":"2024-10-24T17:55:41.49742Z","iopub.execute_input":"2024-10-24T17:55:41.497815Z","iopub.status.idle":"2024-10-24T18:02:50.414484Z","shell.execute_reply.started":"2024-10-24T17:55:41.497786Z","shell.execute_reply":"2024-10-24T18:02:50.412904Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df","metadata":{"execution":{"iopub.status.busy":"2024-10-24T18:02:50.416638Z","iopub.execute_input":"2024-10-24T18:02:50.417043Z","iopub.status.idle":"2024-10-24T18:02:50.432324Z","shell.execute_reply.started":"2024-10-24T18:02:50.41701Z","shell.execute_reply":"2024-10-24T18:02:50.430915Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Duplicated tweets after cleaning","metadata":{}},{"cell_type":"code","source":"print(f'{int(df[\"text_clean\"].duplicated().sum())} duplicated tweets is removed.')\ndf.drop_duplicates(\"text_clean\", inplace=True)","metadata":{"execution":{"iopub.status.busy":"2024-10-24T18:03:09.273177Z","iopub.execute_input":"2024-10-24T18:03:09.274333Z","iopub.status.idle":"2024-10-24T18:03:09.307292Z","shell.execute_reply.started":"2024-10-24T18:03:09.274266Z","shell.execute_reply":"2024-10-24T18:03:09.305674Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df['sentiment'].value_counts()","metadata":{"execution":{"iopub.status.busy":"2024-10-24T18:03:09.79212Z","iopub.execute_input":"2024-10-24T18:03:09.793027Z","iopub.status.idle":"2024-10-24T18:03:09.805883Z","shell.execute_reply.started":"2024-10-24T18:03:09.792989Z","shell.execute_reply":"2024-10-24T18:03:09.804665Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"As shown, except for the other_cyberbullying class, all classes have approximately a balanced number of rows.","metadata":{}},{"cell_type":"markdown","source":"<a name='2'></a>\n\n# Analyzing the dataset","metadata":{}},{"cell_type":"markdown","source":"### Define n-gram and wordcloud visualization functions","metadata":{}},{"cell_type":"code","source":"def get_top_n_gram(corpus, ngram_range, n=None):\n    # Initializing a CountVectorizer with specified n-gram range\n    vec = CountVectorizer(ngram_range=ngram_range).fit(corpus)\n    # Transforming the corpus into a bag of words representation\n    bag_of_words = vec.transform(corpus)\n    sum_words = bag_of_words.sum(axis=0)\n    # Creating a list of tuples containing words and their frequencies\n    words_freq = [(word, sum_words[0, idx]) for word, idx in vec.vocabulary_.items()]\n    # Sorting the list of tuples by frequency in descending order\n    words_freq = sorted(words_freq, key=lambda x: x[1], reverse=True)\n    return words_freq[:n]","metadata":{"execution":{"iopub.status.busy":"2024-10-24T18:03:10.928165Z","iopub.execute_input":"2024-10-24T18:03:10.928616Z","iopub.status.idle":"2024-10-24T18:03:10.936027Z","shell.execute_reply.started":"2024-10-24T18:03:10.928581Z","shell.execute_reply":"2024-10-24T18:03:10.934718Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def generate_wordcloud(sentiment, df):\n    plt.figure(figsize=(20, 10))\n    subset = df[df['sentiment'] == sentiment]\n    text_sentiment = subset.text_clean.values\n    cloud = WordCloud(background_color='black', colormap=\"Dark2\", collocations=False, width=2000, height=1000).generate(\" \".join(text_sentiment))\n    \n    plt.axis('off')\n    plt.title(sentiment.capitalize(), fontsize=40)\n    plt.imshow(cloud)\n    \n    plt.savefig(f\"{sentiment}_wordcloud.svg\", format='svg', bbox_inches='tight')\n    plt.show()","metadata":{"execution":{"iopub.status.busy":"2024-10-24T18:03:11.348789Z","iopub.execute_input":"2024-10-24T18:03:11.349282Z","iopub.status.idle":"2024-10-24T18:03:11.357947Z","shell.execute_reply.started":"2024-10-24T18:03:11.349232Z","shell.execute_reply":"2024-10-24T18:03:11.35656Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def plot_ngrams(sentiment, df):\n    subset = df[df['sentiment'] == sentiment]\n    text_sentiment = subset.text_clean.values\n    \n    # Calculating the top unigrams and bigrams\n    unigrams = get_top_n_gram(text_sentiment, (1, 1), 10)\n    bigrams = get_top_n_gram(text_sentiment, (2, 2), 10)\n\n    # Creating DataFrames from the top unigrams and bigrams\n    unigrams_df = pd.DataFrame(unigrams, columns=['text_clean', 'count'])\n    bigrams_df = pd.DataFrame(bigrams, columns=['text_clean', 'count'])\n\n    # Grouping by 'Text', summing the counts, and sorting the values in ascending order\n    unigrams_grouped = unigrams_df.groupby('text_clean').sum()['count'].sort_values(ascending=True)\n    bigrams_grouped = bigrams_df.groupby('text_clean').sum()['count'].sort_values(ascending=True)\n\n    # Converting the Series objects to DataFrames\n    unigrams_df = unigrams_grouped.reset_index()\n    bigrams_df = bigrams_grouped.reset_index()\n\n\n    # Creating subplots with two bar plots\n    fig, axes = plt.subplots(1, 2, figsize=(14, 6))\n\n    # top 10 unigrams\n    sns.barplot(x=\"count\", y=\"text_clean\", data=unigrams_df, ax=axes[0], palette=\"viridis\", edgecolor = 'black')\n    axes[0].set_xlabel(\"Count\")\n    axes[0].set_ylabel(\"Unigrams\")\n    axes[0].set_title(\"Top 10 Unigrams\")\n    axes[0].xaxis.grid(True, alpha=0.3)\n\n\n    # top 10 bigrams\n    sns.barplot(x=\"count\", y=\"text_clean\", data=bigrams_df, ax=axes[1], palette=\"viridis\", edgecolor = 'black')\n    axes[1].set_xlabel(\"Count\")\n    axes[1].set_ylabel(\"Bigrams\")\n    axes[1].set_title(\"Top 10 Bigrams\")\n    axes[1].xaxis.grid(True, alpha=0.3)\n\n\n    # Showing plot\n    plt.tight_layout()\n    plt.suptitle(f'Top 10 Ngrams in {sentiment.capitalize()}', fontsize=16)\n    #plt.savefig(f\"{sentiment}_ngrams.svg\", format='svg', bbox_inches='tight')\n    plt.show()","metadata":{"execution":{"iopub.status.busy":"2024-10-24T18:03:11.665458Z","iopub.execute_input":"2024-10-24T18:03:11.666266Z","iopub.status.idle":"2024-10-24T18:03:11.67894Z","shell.execute_reply.started":"2024-10-24T18:03:11.666212Z","shell.execute_reply":"2024-10-24T18:03:11.677467Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"<a name='2.1'></a>\n\n## Category-wise Analysis","metadata":{}},{"cell_type":"markdown","source":"### Religion","metadata":{}},{"cell_type":"code","source":"generate_wordcloud('religion', df)\nplot_ngrams('religion', df)","metadata":{"execution":{"iopub.status.busy":"2024-10-24T18:23:12.678341Z","iopub.execute_input":"2024-10-24T18:23:12.678819Z","iopub.status.idle":"2024-10-24T18:23:20.141215Z","shell.execute_reply.started":"2024-10-24T18:23:12.678784Z","shell.execute_reply":"2024-10-24T18:23:20.139893Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Age","metadata":{}},{"cell_type":"code","source":"generate_wordcloud('age', df)\nplot_ngrams('age', df)","metadata":{"execution":{"iopub.status.busy":"2024-10-24T18:24:20.184779Z","iopub.execute_input":"2024-10-24T18:24:20.18527Z","iopub.status.idle":"2024-10-24T18:24:27.417545Z","shell.execute_reply.started":"2024-10-24T18:24:20.185217Z","shell.execute_reply":"2024-10-24T18:24:27.416293Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Ethnicity","metadata":{}},{"cell_type":"code","source":"generate_wordcloud('ethnicity', df)\nplot_ngrams('ethnicity', df)","metadata":{"execution":{"iopub.status.busy":"2024-10-24T17:14:17.197951Z","iopub.execute_input":"2024-10-24T17:14:17.198462Z","iopub.status.idle":"2024-10-24T17:14:24.813158Z","shell.execute_reply.started":"2024-10-24T17:14:17.198418Z","shell.execute_reply":"2024-10-24T17:14:24.811822Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Gender","metadata":{}},{"cell_type":"code","source":"generate_wordcloud('gender', df)\nplot_ngrams('gender', df)","metadata":{"execution":{"iopub.status.busy":"2024-10-24T18:24:55.26247Z","iopub.execute_input":"2024-10-24T18:24:55.262872Z","iopub.status.idle":"2024-10-24T18:25:02.766624Z","shell.execute_reply.started":"2024-10-24T18:24:55.262843Z","shell.execute_reply":"2024-10-24T18:25:02.765215Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Other_cyberbullying","metadata":{}},{"cell_type":"code","source":"generate_wordcloud('other_cyberbullying', df)\nplot_ngrams('other_cyberbullying', df)","metadata":{"execution":{"iopub.status.busy":"2024-10-24T18:26:31.502015Z","iopub.execute_input":"2024-10-24T18:26:31.502482Z","iopub.status.idle":"2024-10-24T18:26:38.62408Z","shell.execute_reply.started":"2024-10-24T18:26:31.502448Z","shell.execute_reply":"2024-10-24T18:26:38.622872Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Not_cyberbullying","metadata":{}},{"cell_type":"code","source":"generate_wordcloud('not_cyberbullying', df)\nplot_ngrams('not_cyberbullying', df)","metadata":{"execution":{"iopub.status.busy":"2024-10-24T18:26:38.626209Z","iopub.execute_input":"2024-10-24T18:26:38.627217Z","iopub.status.idle":"2024-10-24T18:26:46.064161Z","shell.execute_reply.started":"2024-10-24T18:26:38.627175Z","shell.execute_reply":"2024-10-24T18:26:46.062802Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"As mentioned, a significant number of tweets belonging to the class \"other_cyberbullying\" have been observed to be removed. Due to the highly unbalanced nature of this class compared to others and its generic nature, a decision has been made to remove tweets labeled as belonging to this class.\n\nNote: It has been noted that by performing some tests, the f1 score for predicting the \"other_cyberbullying\" class resulted to be around 60%, a value far lower compared to the other f1 scores (around 92% using LSTM model). This supports the decision to remove this class.","metadata":{}},{"cell_type":"code","source":"# Dropping the other_cyberBulling class\ndf = df[df[\"sentiment\"]!=\"other_cyberbullying\"]","metadata":{"execution":{"iopub.status.busy":"2024-10-24T18:38:14.239829Z","iopub.execute_input":"2024-10-24T18:38:14.240317Z","iopub.status.idle":"2024-10-24T18:38:14.257209Z","shell.execute_reply.started":"2024-10-24T18:38:14.240281Z","shell.execute_reply":"2024-10-24T18:38:14.255911Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#  5 sentiments exist in the dataset\nsentiments = [\"religion\",\"age\",\"ethnicity\",\"gender\",\"not bullying\"]","metadata":{"execution":{"iopub.status.busy":"2024-10-24T18:38:20.690159Z","iopub.execute_input":"2024-10-24T18:38:20.69057Z","iopub.status.idle":"2024-10-24T18:38:20.696038Z","shell.execute_reply.started":"2024-10-24T18:38:20.690541Z","shell.execute_reply":"2024-10-24T18:38:20.694695Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"<a name='2.2'></a>\n\n## Tweets length analysis","metadata":{}},{"cell_type":"code","source":"df['text_len'] = [len(text.split()) for text in df.text_clean]","metadata":{"execution":{"iopub.status.busy":"2024-10-24T18:38:22.454388Z","iopub.execute_input":"2024-10-24T18:38:22.4548Z","iopub.status.idle":"2024-10-24T18:38:22.517321Z","shell.execute_reply.started":"2024-10-24T18:38:22.454769Z","shell.execute_reply":"2024-10-24T18:38:22.515746Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Sorting the rows based on the their lenths\ndf.sort_values(by=['text_len'], ascending=False)","metadata":{"execution":{"iopub.status.busy":"2024-10-24T18:38:23.403606Z","iopub.execute_input":"2024-10-24T18:38:23.404199Z","iopub.status.idle":"2024-10-24T18:38:23.429741Z","shell.execute_reply.started":"2024-10-24T18:38:23.404163Z","shell.execute_reply":"2024-10-24T18:38:23.42848Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"plt.figure(figsize=(20,5))\nax = sns.countplot(x='text_len', data=df, palette='viridis')\nplt.title('Count of tweets with high number of words', fontsize=25)\nax.bar_label(ax.containers[0])\nplt.ylabel('Count', fontsize=20)\nplt.xlabel('Tweet Length', fontsize=20)\nax.yaxis.grid(True, alpha=0.3)\n\n# Calculate quartiles\nq1 = np.percentile(df['text_len'], 25)\nq2 = np.percentile(df['text_len'], 50)\nq3 = np.percentile(df['text_len'], 75)\nq4 = np.percentile(df['text_len'], 99)\n\n\n# Add lines for quartiles\nplt.axvline(x=q2, color='green', linestyle='--', label='50th Percentile (Median)')\nplt.axvline(x=q3, color='blue', linestyle='--', label='75th Percentile')\nplt.axvline(x=q4, color='red', linestyle='--', label='99th Percentile')\n\n\nplt.legend(fontsize = 18)\n\n# Save the plot as SVG\n#plt.savefig('tweet_length_plot_with_quartiles.svg', format='svg', bbox_inches='tight')\n\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2024-10-24T18:38:23.909034Z","iopub.execute_input":"2024-10-24T18:38:23.909464Z","iopub.status.idle":"2024-10-24T18:38:24.681381Z","shell.execute_reply.started":"2024-10-24T18:38:23.909429Z","shell.execute_reply":"2024-10-24T18:38:24.679946Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"To organise computational time, a new variable called text_len introduced to represent the word count for each tweet. Figure highlights tweet length distribution, indicating potential outliers beyond the 99th percentile. To optimize efficiency, rows exceeding the 0.99 quantile are removed. As a result, tweets with lengths exceeding 31 are eliminated, and the maximum length for all tweets is capped at 31.","metadata":{}},{"cell_type":"code","source":"df = df[df['text_len'] < df['text_len'].quantile(0.995)]","metadata":{"execution":{"iopub.status.busy":"2024-10-24T18:38:25.180112Z","iopub.execute_input":"2024-10-24T18:38:25.18052Z","iopub.status.idle":"2024-10-24T18:38:25.195519Z","shell.execute_reply.started":"2024-10-24T18:38:25.180491Z","shell.execute_reply":"2024-10-24T18:38:25.194268Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df['sentiment'].value_counts()","metadata":{"execution":{"iopub.status.busy":"2024-10-24T18:38:25.847087Z","iopub.execute_input":"2024-10-24T18:38:25.84754Z","iopub.status.idle":"2024-10-24T18:38:25.859516Z","shell.execute_reply.started":"2024-10-24T18:38:25.847507Z","shell.execute_reply":"2024-10-24T18:38:25.858088Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"The length of the longest tweet is also obtained since it will be useful later.","metadata":{}},{"cell_type":"code","source":"max_len = np.max(df['text_len'])\nmax_len ","metadata":{"execution":{"iopub.status.busy":"2024-10-24T18:38:35.662607Z","iopub.execute_input":"2024-10-24T18:38:35.663026Z","iopub.status.idle":"2024-10-24T18:38:35.671312Z","shell.execute_reply.started":"2024-10-24T18:38:35.662994Z","shell.execute_reply":"2024-10-24T18:38:35.669949Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df.sort_values(by=[\"text_len\"], ascending=False)","metadata":{"execution":{"iopub.status.busy":"2024-10-24T18:38:37.789404Z","iopub.execute_input":"2024-10-24T18:38:37.789846Z","iopub.status.idle":"2024-10-24T18:38:37.8153Z","shell.execute_reply.started":"2024-10-24T18:38:37.789814Z","shell.execute_reply":"2024-10-24T18:38:37.814153Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"plt.figure(figsize=(16,5))\nax = sns.countplot(x='text_len', data=df, edgecolor = 'black',palette='viridis')\nplt.title('Count of tweets with high number of words', fontsize=25)\nax.bar_label(ax.containers[0])\nplt.ylabel('count', fontsize = 17)\nplt.xlabel('Tweet Length', fontsize = 17)\nax.yaxis.grid(True, alpha = 0.3)\n#plt.savefig('tweet_length_plot_after_filter.svg', format='svg', bbox_inches='tight')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2024-10-24T18:38:39.553298Z","iopub.execute_input":"2024-10-24T18:38:39.553672Z","iopub.status.idle":"2024-10-24T18:38:40.178025Z","shell.execute_reply.started":"2024-10-24T18:38:39.553645Z","shell.execute_reply":"2024-10-24T18:38:40.176914Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"<a name='3'></a>\n\n# Traditional medels","metadata":{}},{"cell_type":"markdown","source":"### Sentiment column encoding","metadata":{}},{"cell_type":"markdown","source":"The target column will be encoded\n","metadata":{}},{"cell_type":"code","source":"df['sentiment_code'] = df['sentiment'].replace({'religion':0,'age':1,'ethnicity':2,'gender':3,'not_cyberbullying':4})","metadata":{"execution":{"iopub.status.busy":"2024-10-24T18:38:48.748416Z","iopub.execute_input":"2024-10-24T18:38:48.748889Z","iopub.status.idle":"2024-10-24T18:38:48.776055Z","shell.execute_reply.started":"2024-10-24T18:38:48.748854Z","shell.execute_reply":"2024-10-24T18:38:48.774693Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Train - Test split","metadata":{}},{"cell_type":"code","source":"df['sentiment_code'].unique()","metadata":{"execution":{"iopub.status.busy":"2024-10-24T18:38:50.817137Z","iopub.execute_input":"2024-10-24T18:38:50.817555Z","iopub.status.idle":"2024-10-24T18:38:50.826452Z","shell.execute_reply.started":"2024-10-24T18:38:50.817524Z","shell.execute_reply":"2024-10-24T18:38:50.825209Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"X = df['text_clean']\ny = df['sentiment_code']","metadata":{"execution":{"iopub.status.busy":"2024-10-24T18:38:52.087468Z","iopub.execute_input":"2024-10-24T18:38:52.088438Z","iopub.status.idle":"2024-10-24T18:38:52.093565Z","shell.execute_reply.started":"2024-10-24T18:38:52.0884Z","shell.execute_reply":"2024-10-24T18:38:52.092344Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# using shuffle to ensure that each set (training and testing) contains a representative sample of each category\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, stratify=y, random_state=seed_value, shuffle=True)","metadata":{"execution":{"iopub.status.busy":"2024-10-24T18:38:56.776792Z","iopub.execute_input":"2024-10-24T18:38:56.777568Z","iopub.status.idle":"2024-10-24T18:38:56.80109Z","shell.execute_reply.started":"2024-10-24T18:38:56.777528Z","shell.execute_reply":"2024-10-24T18:38:56.799825Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"(unique, counts) = np.unique(y_train, return_counts=True)\nnp.asarray((unique, counts)).T","metadata":{"execution":{"iopub.status.busy":"2024-10-24T18:39:10.609419Z","iopub.execute_input":"2024-10-24T18:39:10.609875Z","iopub.status.idle":"2024-10-24T18:39:10.619422Z","shell.execute_reply.started":"2024-10-24T18:39:10.609841Z","shell.execute_reply":"2024-10-24T18:39:10.618178Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"The classes are not completely balanced, so it could be a good idea to oversample the training set such that all classes have the same count as the most populated one. The RandomOverSampler in Python's imbalanced-learn library is used to balance the class distribution by randomly duplicating samples from the minority class. Given the moderate level of imbalance, oversampling seems like an appropriate approach that shouldn't lead to overfitting issues and does not have the problem of sampling in NLP tasks.","metadata":{}},{"cell_type":"markdown","source":"### Oversampling of training set","metadata":{}},{"cell_type":"code","source":"y_train.value_counts()","metadata":{"execution":{"iopub.status.busy":"2024-10-24T18:39:16.966945Z","iopub.execute_input":"2024-10-24T18:39:16.967424Z","iopub.status.idle":"2024-10-24T18:39:16.977085Z","shell.execute_reply.started":"2024-10-24T18:39:16.967387Z","shell.execute_reply":"2024-10-24T18:39:16.97559Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# using RAndomoverSampler to completely balance the data\nros = RandomOverSampler()\nX_train, y_train = ros.fit_resample(np.array(X_train).reshape(-1, 1), np.array(y_train).reshape(-1, 1));\ntrain_os = pd.DataFrame(list(zip([x[0] for x in X_train], y_train)), columns = ['text_clean', 'sentiment']);","metadata":{"execution":{"iopub.status.busy":"2024-10-24T18:39:17.603469Z","iopub.execute_input":"2024-10-24T18:39:17.603849Z","iopub.status.idle":"2024-10-24T18:39:17.707459Z","shell.execute_reply.started":"2024-10-24T18:39:17.60382Z","shell.execute_reply":"2024-10-24T18:39:17.706199Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"X_train = train_os['text_clean'].values\ny_train = train_os['sentiment'].values","metadata":{"execution":{"iopub.status.busy":"2024-10-24T18:39:18.353528Z","iopub.execute_input":"2024-10-24T18:39:18.353937Z","iopub.status.idle":"2024-10-24T18:39:18.360431Z","shell.execute_reply.started":"2024-10-24T18:39:18.353906Z","shell.execute_reply":"2024-10-24T18:39:18.359128Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"(unique, counts) = np.unique(y_train, return_counts=True)\nnp.asarray((unique, counts)).T","metadata":{"execution":{"iopub.status.busy":"2024-10-24T18:39:19.496531Z","iopub.execute_input":"2024-10-24T18:39:19.496947Z","iopub.status.idle":"2024-10-24T18:39:19.506752Z","shell.execute_reply.started":"2024-10-24T18:39:19.496916Z","shell.execute_reply":"2024-10-24T18:39:19.505535Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"First, creating a bag of words using CountVectorizer.","metadata":{}},{"cell_type":"code","source":"clf = CountVectorizer()\nX_train_cv =  clf.fit_transform(X_train)\nX_test_cv = clf.transform(X_test)","metadata":{"execution":{"iopub.status.busy":"2024-10-24T18:39:21.176506Z","iopub.execute_input":"2024-10-24T18:39:21.17694Z","iopub.status.idle":"2024-10-24T18:39:21.862338Z","shell.execute_reply.started":"2024-10-24T18:39:21.176904Z","shell.execute_reply":"2024-10-24T18:39:21.861134Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"The TF-IDF transformation is applied to associate weights to the different words based on their frequency (rarer words will be given more importance).","metadata":{}},{"cell_type":"code","source":"tf_transformer = TfidfTransformer(use_idf=True).fit(X_train_cv)\nX_train_tf = tf_transformer.transform(X_train_cv)\nX_test_tf = tf_transformer.transform(X_test_cv)","metadata":{"execution":{"iopub.status.busy":"2024-10-24T18:39:23.09777Z","iopub.execute_input":"2024-10-24T18:39:23.09826Z","iopub.status.idle":"2024-10-24T18:39:23.159801Z","shell.execute_reply.started":"2024-10-24T18:39:23.098207Z","shell.execute_reply":"2024-10-24T18:39:23.158617Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Grid Search and Evaluation Functions","metadata":{}},{"cell_type":"code","source":"def perform_grid_search(clf, param_grid, X, y):\n    \"\"\"\n    Performing grid search to find the best hyperparameters.\n    \"\"\"\n\n    # Initialize GridSearchCV\n    grid_search = GridSearchCV(estimator=clf, param_grid=param_grid, cv=5, scoring='accuracy',n_jobs=-1)\n\n    # Perform grid search on training data\n    grid_search.fit(X, y)\n\n    # Best parameters and the best accuracy score\n    best_params = grid_search.best_params_\n    best_score = grid_search.best_score_\n    print(\"Best Parameters:\", best_params)\n    print(\"Best Accuracy Score:\", best_score)\n\n    return grid_search","metadata":{"execution":{"iopub.status.busy":"2024-10-24T18:39:24.568784Z","iopub.execute_input":"2024-10-24T18:39:24.569224Z","iopub.status.idle":"2024-10-24T18:39:24.577164Z","shell.execute_reply.started":"2024-10-24T18:39:24.56919Z","shell.execute_reply":"2024-10-24T18:39:24.5759Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def evaluate_classification_with_model(model_name, model, X_test, y_true, sentiments):\n    evaluation_results = {}\n    evaluation_results['Model'] = model_name\n\n    if hasattr(model, \"predict_proba\"):  # traditional models\n        y_pred_probs = model.predict_proba(X_test)\n\n        # Predicted classes\n        y_pred = model.predict(X_test)\n\n    else:  # neural network models\n        # Predicted probabilities\n        y_pred_probs = model.predict(X_test)\n\n        # Predicted classes\n        y_pred = np.argmax(y_pred_probs, axis=1)\n\n    # Accuracy\n    accuracy = accuracy_score(y_true, y_pred)\n    evaluation_results['Accuracy'] = accuracy\n\n    # Precision, Recall, F1-score\n    precision = precision_score(y_true, y_pred, average='macro')\n    recall = recall_score(y_true, y_pred, average='macro')\n    f1 = f1_score(y_true, y_pred, average='macro')\n    evaluation_results['Precision'] = precision\n    evaluation_results['Recall'] = recall\n    evaluation_results['F1-score'] = f1\n\n    # ROC-AUC for multi-class classification\n    roc_auc = []\n    for i in range(len(sentiments)):\n        roc_auc.append(roc_auc_score(y_true == i, y_pred_probs[:, i]))\n    evaluation_results['ROC-AUC'] = np.mean(roc_auc)\n\n    # PR-AUC for multi-class classification\n    pr_auc = []\n    for i in range(len(sentiments)):\n        pr_auc.append(average_precision_score(y_true == i, y_pred_probs[:, i]))\n    evaluation_results['PR-AUC'] = np.mean(pr_auc)\n\n    # ROC curve plotting\n    plt.figure(figsize=(5, 5))\n    for i, sentiment in enumerate(sentiments):\n        fpr, tpr, _ = roc_curve(y_true == i, y_pred_probs[:, i])\n        plt.plot(fpr, tpr, label=f'ROC curve ({sentiment}) (AUC = {roc_auc[i]:0.2f})')\n\n    plt.plot([0, 1], [0, 1], 'k--')\n    plt.xlim([0.0, 1.0])\n    plt.ylim([0.0, 1.05])\n    plt.xlabel('False Positive Rate')\n    plt.ylabel('True Positive Rate')\n    plt.title(f'ROC Curve for {model_name} Classification')\n    plt.legend(loc=\"lower right\", fontsize=12)\n\n    plt.show()\n\n    # Print evaluation metrics with correct formatting\n    for key, value in evaluation_results.items():\n        if isinstance(value, (int, float)):  # Check if the value is numeric\n            print(f\"{key}: {value:.4f}\")\n        else:\n            print(f\"{key}: {value}\")\n\n    return evaluation_results\n","metadata":{"execution":{"iopub.status.busy":"2024-10-24T18:39:25.407683Z","iopub.execute_input":"2024-10-24T18:39:25.408112Z","iopub.status.idle":"2024-10-24T18:39:25.422538Z","shell.execute_reply.started":"2024-10-24T18:39:25.408081Z","shell.execute_reply":"2024-10-24T18:39:25.420968Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def conf_matrix(y, y_pred, title, labels):\n    fig, ax =plt.subplots(figsize=(6.5,6.5))\n    ax=sns.heatmap(confusion_matrix(y, y_pred), annot=True, cmap=\"viridis\", fmt='g', cbar=False, annot_kws={\"size\":20})\n    plt.title(title, fontsize=20)\n    ax.xaxis.set_ticklabels(labels, fontsize=15.5) \n    ax.yaxis.set_ticklabels(labels, fontsize=15.5)\n    ax.set_ylabel('Test', fontsize=15)\n    ax.set_xlabel('Predicted', fontsize=15)\n\n    file_name = f\"{title}_confusion_matrix.svg\"\n    #plt.savefig(file_name, format='svg', bbox_inches='tight')\n    plt.show()","metadata":{"execution":{"iopub.status.busy":"2024-10-24T18:39:27.533576Z","iopub.execute_input":"2024-10-24T18:39:27.533978Z","iopub.status.idle":"2024-10-24T18:39:27.541848Z","shell.execute_reply.started":"2024-10-24T18:39:27.53394Z","shell.execute_reply":"2024-10-24T18:39:27.540557Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"<a name='3.1'></a>\n\n## Random Forest\nAs the dataset is large and the model size is too big for using gridSearch in Random Forest, hyperparameters such as n_estimators, max_depth, and min_samples_split are manually tuned. The parameters that lead to better results than the default settings are selected and used to train the final model.","metadata":{}},{"cell_type":"code","source":"list_of_evaluations = [] # this list of all models' evaluations\nruntime = {} # a dictionary containing all model's runtimes","metadata":{"execution":{"iopub.status.busy":"2024-10-24T18:39:47.333153Z","iopub.execute_input":"2024-10-24T18:39:47.333595Z","iopub.status.idle":"2024-10-24T18:39:47.338687Z","shell.execute_reply.started":"2024-10-24T18:39:47.333562Z","shell.execute_reply":"2024-10-24T18:39:47.337492Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"start_time = time.time()\n\nrf_clf = RandomForestClassifier(n_estimators=150)\nrf_clf.fit(X_train_tf, y_train)\n\nend_time = time.time()\n\n# Calculate the runtime\nruntime['RF'] = end_time - start_time","metadata":{"execution":{"iopub.status.busy":"2024-10-24T18:39:48.397055Z","iopub.execute_input":"2024-10-24T18:39:48.398131Z","iopub.status.idle":"2024-10-24T18:41:24.083558Z","shell.execute_reply.started":"2024-10-24T18:39:48.39809Z","shell.execute_reply":"2024-10-24T18:41:24.082292Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"runtime","metadata":{"execution":{"iopub.status.busy":"2024-10-24T18:41:24.085548Z","iopub.execute_input":"2024-10-24T18:41:24.085925Z","iopub.status.idle":"2024-10-24T18:41:24.092815Z","shell.execute_reply.started":"2024-10-24T18:41:24.085894Z","shell.execute_reply":"2024-10-24T18:41:24.091749Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"rf_pred = rf_clf.predict(X_test_tf)\n\nprint('Classification Report for Random Forest:\\n',classification_report(y_test, rf_pred, target_names=sentiments))\nconf_matrix(y_test,rf_pred,'Random Forest Sentiment Analysis', sentiments)","metadata":{"execution":{"iopub.status.busy":"2024-10-24T18:41:55.564046Z","iopub.execute_input":"2024-10-24T18:41:55.564567Z","iopub.status.idle":"2024-10-24T18:41:56.722039Z","shell.execute_reply.started":"2024-10-24T18:41:55.564534Z","shell.execute_reply":"2024-10-24T18:41:56.720914Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"list_of_evaluations.append(evaluate_classification_with_model('RF', rf_clf, X_test_tf, y_test,sentiments))","metadata":{"execution":{"iopub.status.busy":"2024-10-24T18:42:08.8341Z","iopub.execute_input":"2024-10-24T18:42:08.834527Z","iopub.status.idle":"2024-10-24T18:42:11.008463Z","shell.execute_reply.started":"2024-10-24T18:42:08.834495Z","shell.execute_reply":"2024-10-24T18:42:11.007065Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"<a name='3.2'></a>\n\n## Gradient Boosting Classifier\nAs the dataset is large and the model size is too big for using gridSearch in GB, hyperparameters such as n_estimators and max_depth are manually tuned. The parameters that lead to better results than the default settings are selected and used to train the final model.","metadata":{}},{"cell_type":"code","source":"start_time = time.time()\n\ngb_clf = GradientBoostingClassifier(n_estimators=150)\ngb_clf.fit(X_train_tf, y_train)\nend_time = time.time()\n\n# Calculate the runtime\nruntime['GB'] = end_time - start_time\n\ngb_pred = gb_clf.predict(X_test_tf)\n\nprint('Classification Report for Gradient Boosting:\\n',classification_report(y_test, gb_pred, target_names=sentiments))\nconf_matrix(y_test,gb_pred,'Gradient Boosting Sentiment Analysis', sentiments)","metadata":{"execution":{"iopub.status.busy":"2024-10-24T18:42:22.570342Z","iopub.execute_input":"2024-10-24T18:42:22.570908Z","iopub.status.idle":"2024-10-24T18:45:12.304385Z","shell.execute_reply.started":"2024-10-24T18:42:22.570868Z","shell.execute_reply":"2024-10-24T18:45:12.303182Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"list_of_evaluations.append(evaluate_classification_with_model('GB', gb_clf, X_test_tf, y_test,sentiments))","metadata":{"execution":{"iopub.status.busy":"2024-10-24T18:45:12.306637Z","iopub.execute_input":"2024-10-24T18:45:12.307083Z","iopub.status.idle":"2024-10-24T18:45:12.966573Z","shell.execute_reply.started":"2024-10-24T18:45:12.307044Z","shell.execute_reply":"2024-10-24T18:45:12.965349Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"<a name='3.3'></a>\n\n## Multinomial Naive Bayes classifier\nGridSearch is used to find the best hyper parameters","metadata":{}},{"cell_type":"code","source":"# Defining the parameter grid\nnb_param_grid = {\n    'alpha': [0.1, 0.5, 1.0, 2.0],\n}\n# Initialize Multinomial Naive Bayes classifier\nstart_time = time.time()\nclf = MultinomialNB()\n\nnb_clf = perform_grid_search(clf, nb_param_grid, X_train_tf, y_train)\nend_time = time.time()\nruntime['NB'] = end_time - start_time","metadata":{"execution":{"iopub.status.busy":"2024-10-24T18:45:12.968535Z","iopub.execute_input":"2024-10-24T18:45:12.969406Z","iopub.status.idle":"2024-10-24T18:45:14.605357Z","shell.execute_reply.started":"2024-10-24T18:45:12.969371Z","shell.execute_reply":"2024-10-24T18:45:14.60388Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"nb_pred = nb_clf.predict(X_test_tf)\n\nprint('Classification Report for Naive Bayes:\\n',classification_report(y_test, nb_pred, target_names=sentiments))\nconf_matrix(y_test,nb_pred,'Naive Bayes Sentiment Analysis', sentiments)","metadata":{"execution":{"iopub.status.busy":"2024-10-24T18:45:14.609002Z","iopub.execute_input":"2024-10-24T18:45:14.610327Z","iopub.status.idle":"2024-10-24T18:45:15.015709Z","shell.execute_reply.started":"2024-10-24T18:45:14.610271Z","shell.execute_reply":"2024-10-24T18:45:15.014349Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# evaluation_result = evaluating_new(nb_clf, X_test_tf, y_test, 'Naive Bayes')\nlist_of_evaluations.append(evaluate_classification_with_model('NB', nb_clf, X_test_tf, y_test, sentiments))","metadata":{"execution":{"iopub.status.busy":"2024-10-24T18:45:15.017327Z","iopub.execute_input":"2024-10-24T18:45:15.017701Z","iopub.status.idle":"2024-10-24T18:45:15.528384Z","shell.execute_reply.started":"2024-10-24T18:45:15.017671Z","shell.execute_reply":"2024-10-24T18:45:15.526957Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"<a name='3.4'></a>\n\n## LogisticRegression","metadata":{}},{"cell_type":"code","source":"start_time = time.time()\n\nlr_clf = LogisticRegression(max_iter=1000)\nlr_clf.fit(X_train_tf, y_train)\n\nend_time = time.time()\nruntime['LR'] = end_time - start_time\n\nlr_pred = lr_clf.predict(X_test_tf)\n\nprint('Classification Report for Logistic Regression:\\n',classification_report(y_test, lr_pred, target_names=sentiments))\n\nconf_matrix(y_test,lr_pred,'Logistic Regression Sentiment Analysis', sentiments)","metadata":{"execution":{"iopub.status.busy":"2024-10-24T18:45:15.529944Z","iopub.execute_input":"2024-10-24T18:45:15.530426Z","iopub.status.idle":"2024-10-24T18:45:31.570502Z","shell.execute_reply.started":"2024-10-24T18:45:15.530393Z","shell.execute_reply":"2024-10-24T18:45:31.569139Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"list_of_evaluations.append(evaluate_classification_with_model('LR', lr_clf, X_test_tf, y_test, sentiments))","metadata":{"execution":{"iopub.status.busy":"2024-10-24T18:45:31.572021Z","iopub.execute_input":"2024-10-24T18:45:31.57247Z","iopub.status.idle":"2024-10-24T18:45:32.050639Z","shell.execute_reply.started":"2024-10-24T18:45:31.572431Z","shell.execute_reply":"2024-10-24T18:45:32.049307Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"<a name='3.5'></a>\n\n## SVC\nThe most efficient kernel found for the Support Vector Classifier (SVC) is the sigmoid kernel. It's noteworthy that its performance is comparable to that of the 'rbf' kernel, yet it exhibits significantly lower runtime.","metadata":{}},{"cell_type":"code","source":"#start_time = time.time()\n\nsvm_clf = SVC(kernel='sigmoid')\nsvm_clf.fit(X_train_tf, y_train)\n\n#end_time = time.time()\n#runtime['SVC'] = end_time - start_time\n\n\nsvm_pred = svm_clf.predict(X_test_tf)\n\nprint('Classification Report for SVM:\\n',classification_report(y_test, svm_pred, target_names=sentiments))\n\nconf_matrix(y_test,svm_pred,'SVM Sentiment Analysis', sentiments)","metadata":{"execution":{"iopub.status.busy":"2024-10-24T18:45:32.052307Z","iopub.execute_input":"2024-10-24T18:45:32.052752Z","iopub.status.idle":"2024-10-24T18:46:44.112899Z","shell.execute_reply.started":"2024-10-24T18:45:32.052711Z","shell.execute_reply":"2024-10-24T18:46:44.111489Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"<a name='3.6'></a>\n\n## Stacking","metadata":{}},{"cell_type":"code","source":"def get_stacking():\n    '''\n    Create a stacking classifier\n    '''\n    level0 = []\n    level0.append(('DT', LogisticRegression()))\n    level0.append(('SVC', SVC(kernel='sigmoid')))\n    level0.append(('NB', MultinomialNB(alpha = 0.5)))\n    \n    level1 = LogisticRegression()\n    model = StackingClassifier(estimators=level0, final_estimator=level1, cv = 5)\n    return model","metadata":{"execution":{"iopub.status.busy":"2024-10-24T18:46:44.114556Z","iopub.execute_input":"2024-10-24T18:46:44.115299Z","iopub.status.idle":"2024-10-24T18:46:44.123306Z","shell.execute_reply.started":"2024-10-24T18:46:44.115224Z","shell.execute_reply":"2024-10-24T18:46:44.121589Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"start_time = time.time()\n\nstacking_model = get_stacking();\n\nstacking_model.fit(X_train_tf, y_train)\n\nend_time = time.time()\nruntime['Stacking'] = end_time - start_time\n","metadata":{"execution":{"iopub.status.busy":"2024-10-24T18:46:44.12765Z","iopub.execute_input":"2024-10-24T18:46:44.128046Z","iopub.status.idle":"2024-10-24T18:52:32.209163Z","shell.execute_reply.started":"2024-10-24T18:46:44.128016Z","shell.execute_reply":"2024-10-24T18:52:32.207893Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"stacking_pred = stacking_model.predict(X_test_tf)\n\nprint('Classification Report for stacking_model:\\n',classification_report(y_test, stacking_pred, target_names=sentiments))\n\nconf_matrix(y_test,stacking_pred,'stacking_model Sentiment Analysis', sentiments)","metadata":{"execution":{"iopub.status.busy":"2024-10-24T18:52:32.211186Z","iopub.execute_input":"2024-10-24T18:52:32.21258Z","iopub.status.idle":"2024-10-24T18:52:47.034592Z","shell.execute_reply.started":"2024-10-24T18:52:32.212529Z","shell.execute_reply":"2024-10-24T18:52:47.0335Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"list_of_evaluations.append(evaluate_classification_with_model('Stacking', stacking_model, X_test_tf, y_test,sentiments))","metadata":{"execution":{"iopub.status.busy":"2024-10-24T18:52:47.036Z","iopub.execute_input":"2024-10-24T18:52:47.036374Z","iopub.status.idle":"2024-10-24T18:53:16.277088Z","shell.execute_reply.started":"2024-10-24T18:52:47.036336Z","shell.execute_reply":"2024-10-24T18:53:16.275838Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"<a name='3.7'></a>\n\n## Comparing the evaluations of traditional models","metadata":{}},{"cell_type":"code","source":"evaluation_df = pd.DataFrame(list_of_evaluations)","metadata":{"execution":{"iopub.status.busy":"2024-10-24T18:53:16.278869Z","iopub.execute_input":"2024-10-24T18:53:16.279361Z","iopub.status.idle":"2024-10-24T18:53:16.286629Z","shell.execute_reply.started":"2024-10-24T18:53:16.279319Z","shell.execute_reply":"2024-10-24T18:53:16.285223Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"evaluation_df.set_index('Model', inplace=True)\nevaluation_df","metadata":{"execution":{"iopub.status.busy":"2024-10-24T18:53:16.288233Z","iopub.execute_input":"2024-10-24T18:53:16.288681Z","iopub.status.idle":"2024-10-24T18:53:16.311143Z","shell.execute_reply.started":"2024-10-24T18:53:16.288642Z","shell.execute_reply":"2024-10-24T18:53:16.309792Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Plot all evaluation metrics for all models\nfor metric in evaluation_df.columns:\n    plt.plot(evaluation_df.index, evaluation_df[metric], marker='o', label=metric)\n\n# Customize the plot\nplt.xlabel('Model')\nplt.ylabel('Score')\nplt.title('Evaluation Metrics for Different Models')\nplt.legend()\n#plt.xticks(rotation=45)  \nplt.grid(True, alpha = 0.3)  \nplt.tight_layout() \n\n#plt.savefig('evaluation_metrics_plot.svg', format='svg')\n\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2024-10-24T18:53:16.312534Z","iopub.execute_input":"2024-10-24T18:53:16.312874Z","iopub.status.idle":"2024-10-24T18:53:16.711147Z","shell.execute_reply.started":"2024-10-24T18:53:16.312846Z","shell.execute_reply":"2024-10-24T18:53:16.709899Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"runtime","metadata":{"execution":{"iopub.status.busy":"2024-10-24T18:53:16.712684Z","iopub.execute_input":"2024-10-24T18:53:16.713216Z","iopub.status.idle":"2024-10-24T18:53:16.720943Z","shell.execute_reply.started":"2024-10-24T18:53:16.713175Z","shell.execute_reply":"2024-10-24T18:53:16.71984Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"<a name='4'></a>\n\n# Deep Learning Models","metadata":{}},{"cell_type":"markdown","source":"### Function for Plotting Accuracy Across Epochs","metadata":{}},{"cell_type":"code","source":"def plotting_funct(history_df, name_of_model):\n    '''\n    This function plots training and validation loss, as well as training and validation accuracy, versus epoch.\n\n    Parameters:\n        - history_df: DataFrame containing the training history.\n        - name_of_model: Name of the model for saving the figure.\n\n    Returns:\n        None\n    '''\n    # Using matplotlib to create a figure with 1x2 sub-plots\n    fig, axes = plt.subplots(1, 2, figsize=(8, 4))\n\n    # Plot 1: the training and validation losses versus epoch.\n    axes[0].plot(history_df[\"loss\"], label=\"Training\")\n    axes[0].plot(history_df[\"val_loss\"], label=\"Validation\")\n    # Add a legend to the plot.\n    axes[0].legend()\n    # Label both axes.\n    axes[0].set_xlabel(\"Epoch\")\n    axes[0].set_ylabel(\"Loss = Cross Entropy\")\n    # Add title to the plot.\n    axes[0].set_title('Losses versus Epoch')\n    axes[0].grid(True, alpha=0.2)\n    axes[0].set_ylim(0, 1)\n\n    # Add small dots to the loss plot\n    axes[0].scatter(history_df.index, history_df[\"loss\"], c='blue', s=5, alpha=0.5)\n    axes[0].scatter(history_df.index, history_df[\"val_loss\"], c='darkorange', s=5, alpha=0.5)\n\n    # Plot 2: the training and validation accuracy versus epoch.\n    axes[1].plot(history_df[\"accuracy\"], label=\"Training\")\n    axes[1].plot(history_df[\"val_accuracy\"], label=\"Validation\")\n    # Add a legend to the plot.\n    plt.legend()\n    # Label both axes.\n    axes[1].set_xlabel(\"Epoch\")\n    axes[1].set_ylabel(\"Accuracy\")\n    # Add title to the plot.\n    axes[1].set_title('Accuracy versus Epoch')\n    axes[1].grid(True, alpha=0.2)\n    axes[1].set_ylim(0, 1)\n\n    # Add small dots to the accuracy plot\n    axes[1].scatter(history_df.index, history_df[\"accuracy\"], c='blue', s=5, alpha=0.5)\n    axes[1].scatter(history_df.index, history_df[\"val_accuracy\"], c='darkorange', s=5, alpha=0.5)\n\n    # Saving and showing the figure\n    plt.tight_layout()\n    #plt.savefig(f'{name_of_model}_val_and_accuracy_vs_epochs.svg', format='svg', bbox_inches='tight')\n    plt.show()","metadata":{"execution":{"iopub.status.busy":"2024-10-24T19:00:44.525451Z","iopub.execute_input":"2024-10-24T19:00:44.526608Z","iopub.status.idle":"2024-10-24T19:00:44.539481Z","shell.execute_reply.started":"2024-10-24T19:00:44.52657Z","shell.execute_reply":"2024-10-24T19:00:44.538222Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Preparing the data","metadata":{}},{"cell_type":"code","source":"# Splitting the train data to train and validation\nX_train, X_valid, y_train, y_valid = train_test_split(X_train, y_train, test_size=0.2, stratify=y_train, random_state=seed_value)","metadata":{"execution":{"iopub.status.busy":"2024-10-24T19:03:08.634994Z","iopub.execute_input":"2024-10-24T19:03:08.635389Z","iopub.status.idle":"2024-10-24T19:03:08.659156Z","shell.execute_reply.started":"2024-10-24T19:03:08.63536Z","shell.execute_reply":"2024-10-24T19:03:08.657815Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"(unique, counts) = np.unique(y_train, return_counts=True)\nnp.asarray((unique, counts)).T","metadata":{"execution":{"iopub.status.busy":"2024-10-24T19:03:11.995692Z","iopub.execute_input":"2024-10-24T19:03:11.996119Z","iopub.status.idle":"2024-10-24T19:03:12.004858Z","shell.execute_reply.started":"2024-10-24T19:03:11.996085Z","shell.execute_reply":"2024-10-24T19:03:12.003704Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Tokenization\ntokenizer = Tokenizer()\ntokenizer.fit_on_texts(df['text_clean'])\nX_train_sequences = tokenizer.texts_to_sequences(X_train)\nX_test_sequences  = tokenizer.texts_to_sequences(X_test)\nX_valid_sequences  = tokenizer.texts_to_sequences(X_valid)","metadata":{"execution":{"iopub.status.busy":"2024-10-24T19:03:23.027238Z","iopub.execute_input":"2024-10-24T19:03:23.027674Z","iopub.status.idle":"2024-10-24T19:03:24.54222Z","shell.execute_reply.started":"2024-10-24T19:03:23.02764Z","shell.execute_reply":"2024-10-24T19:03:24.540879Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"num_unique_tokens = len(tokenizer.word_index)\nprint(\"Number of unique tokens:\", num_unique_tokens)","metadata":{"execution":{"iopub.status.busy":"2024-10-24T19:03:29.486958Z","iopub.execute_input":"2024-10-24T19:03:29.487432Z","iopub.status.idle":"2024-10-24T19:03:29.494356Z","shell.execute_reply.started":"2024-10-24T19:03:29.487395Z","shell.execute_reply":"2024-10-24T19:03:29.492847Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"token_lengths = [len(sequence) for sequence in X_train_sequences]\nmean_length   = np.mean(token_lengths)\nmedian_length = np.median(token_lengths)\nmax_length    = int(mean_length + 2 * np.std(token_lengths))\n\nprint(f\"Mean Token Length   : {mean_length}\")\nprint(f\"Median Token Length : {median_length}\")\nprint(f\"Max Token Length    : {max_length}\")","metadata":{"execution":{"iopub.status.busy":"2024-10-24T19:03:37.445975Z","iopub.execute_input":"2024-10-24T19:03:37.446416Z","iopub.status.idle":"2024-10-24T19:03:37.465279Z","shell.execute_reply.started":"2024-10-24T19:03:37.446381Z","shell.execute_reply":"2024-10-24T19:03:37.463562Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"max_len","metadata":{"execution":{"iopub.status.busy":"2024-10-24T19:03:43.201631Z","iopub.execute_input":"2024-10-24T19:03:43.202144Z","iopub.status.idle":"2024-10-24T19:03:43.21069Z","shell.execute_reply.started":"2024-10-24T19:03:43.202101Z","shell.execute_reply":"2024-10-24T19:03:43.209218Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Padding\nmax_length     = 31\nX_train_padded = pad_sequences(X_train_sequences, maxlen=max_length, padding='post')\nX_test_padded  = pad_sequences(X_test_sequences , maxlen=max_length, padding='post')\nX_valid_padded  = pad_sequences(X_valid_sequences , maxlen=max_length, padding='post')","metadata":{"execution":{"iopub.status.busy":"2024-10-24T19:03:52.985637Z","iopub.execute_input":"2024-10-24T19:03:52.986038Z","iopub.status.idle":"2024-10-24T19:03:53.109569Z","shell.execute_reply.started":"2024-10-24T19:03:52.986008Z","shell.execute_reply":"2024-10-24T19:03:53.108266Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"X_test_padded","metadata":{"execution":{"iopub.status.busy":"2024-10-24T19:03:59.974874Z","iopub.execute_input":"2024-10-24T19:03:59.975303Z","iopub.status.idle":"2024-10-24T19:03:59.98344Z","shell.execute_reply.started":"2024-10-24T19:03:59.975264Z","shell.execute_reply":"2024-10-24T19:03:59.982152Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"<a name='4.1'></a>\n\n## Simple LSTM Model","metadata":{}},{"cell_type":"code","source":"# Build the LSTM model\nembedding_dim = 200\nnum_classes   = 5","metadata":{"execution":{"iopub.status.busy":"2024-10-24T19:04:16.786949Z","iopub.execute_input":"2024-10-24T19:04:16.787423Z","iopub.status.idle":"2024-10-24T19:04:16.793202Z","shell.execute_reply.started":"2024-10-24T19:04:16.787389Z","shell.execute_reply":"2024-10-24T19:04:16.792Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"early_stopping = EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)","metadata":{"execution":{"iopub.status.busy":"2024-10-24T19:04:21.888109Z","iopub.execute_input":"2024-10-24T19:04:21.888553Z","iopub.status.idle":"2024-10-24T19:04:21.89423Z","shell.execute_reply.started":"2024-10-24T19:04:21.888518Z","shell.execute_reply":"2024-10-24T19:04:21.892934Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"model = Sequential([\n    Embedding(input_dim=len(tokenizer.word_index) + 1, output_dim=embedding_dim, input_length=max_len),\n    LSTM(units=100, dropout=0.2, recurrent_dropout=0.2),\n    Dense(units=num_classes, activation='softmax')\n])\n\nmodel.summary()","metadata":{"execution":{"iopub.status.busy":"2024-10-24T19:04:27.466396Z","iopub.execute_input":"2024-10-24T19:04:27.466777Z","iopub.status.idle":"2024-10-24T19:04:27.54447Z","shell.execute_reply.started":"2024-10-24T19:04:27.466748Z","shell.execute_reply":"2024-10-24T19:04:27.543424Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n\nhistory = model.fit(X_train_padded, y_train, epochs=10, batch_size=32, validation_data=(X_valid_padded, y_valid), callbacks=[early_stopping])","metadata":{"execution":{"iopub.status.busy":"2024-10-24T19:04:47.546473Z","iopub.execute_input":"2024-10-24T19:04:47.547107Z","iopub.status.idle":"2024-10-24T19:10:07.585525Z","shell.execute_reply.started":"2024-10-24T19:04:47.546937Z","shell.execute_reply":"2024-10-24T19:10:07.584061Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"history_df = pd.DataFrame(history.history)\nplot = plotting_funct(history_df, 'Simple LSTM')","metadata":{"execution":{"iopub.status.busy":"2024-10-24T19:12:54.264042Z","iopub.execute_input":"2024-10-24T19:12:54.264641Z","iopub.status.idle":"2024-10-24T19:12:54.819177Z","shell.execute_reply.started":"2024-10-24T19:12:54.264604Z","shell.execute_reply":"2024-10-24T19:12:54.81816Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"y_pred_probs = model.predict(X_test_padded)\ny_pred = np.argmax(y_pred_probs, axis=1)\n\n\nprint('Classification Report for LSTM:\\n',classification_report(y_test, y_pred, target_names=sentiments))\n\nconf_matrix(y_test,y_pred,'LSTM', sentiments)","metadata":{"execution":{"iopub.status.busy":"2024-10-24T19:13:10.80375Z","iopub.execute_input":"2024-10-24T19:13:10.804261Z","iopub.status.idle":"2024-10-24T19:13:14.429418Z","shell.execute_reply.started":"2024-10-24T19:13:10.804211Z","shell.execute_reply":"2024-10-24T19:13:14.428088Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"<a name='4.2'></a>\n\n## Fine Tuned LSTM","metadata":{}},{"cell_type":"code","source":"embedding_dim = 200\n\n\nstart_time = time.time()\nmodel = Sequential([\n    Embedding(input_dim=len(tokenizer.word_index) + 1, output_dim=embedding_dim, input_length=max_len),\n    LSTM(units=100, dropout=0.5, recurrent_dropout=0.5), #return_sequences=True\n    BatchNormalization(momentum=0.9),\n    Dense(units=num_classes, activation='softmax')\n])\nmodel.summary()\n\nmodel.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n\nhistory = model.fit(X_train_padded, y_train, epochs=10, batch_size=512, validation_data=(X_valid_padded, y_valid), callbacks=[early_stopping])\n\nend_time = time.time()\nruntime['LSTM'] = end_time - start_time","metadata":{"execution":{"iopub.status.busy":"2024-10-24T19:13:41.977863Z","iopub.execute_input":"2024-10-24T19:13:41.978318Z","iopub.status.idle":"2024-10-24T19:14:17.252019Z","shell.execute_reply.started":"2024-10-24T19:13:41.978283Z","shell.execute_reply":"2024-10-24T19:14:17.250748Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"history_df = pd.DataFrame(history.history)\nplot = plotting_funct(history_df, 'Simple LSTM')","metadata":{"execution":{"iopub.status.busy":"2024-10-24T19:20:41.456568Z","iopub.execute_input":"2024-10-24T19:20:41.457021Z","iopub.status.idle":"2024-10-24T19:20:42.097833Z","shell.execute_reply.started":"2024-10-24T19:20:41.45699Z","shell.execute_reply":"2024-10-24T19:20:42.096618Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"y_pred_probs = model.predict(X_test_padded)\ny_pred = np.argmax(y_pred_probs, axis=1)\n\n\nprint('Classification Report for LSTM:\\n',classification_report(y_test, y_pred, target_names=sentiments))\n\nconf_matrix(y_test,y_pred,'LSTM', sentiments)","metadata":{"execution":{"iopub.status.busy":"2024-10-24T19:21:53.871382Z","iopub.execute_input":"2024-10-24T19:21:53.871778Z","iopub.status.idle":"2024-10-24T19:21:57.354446Z","shell.execute_reply.started":"2024-10-24T19:21:53.87175Z","shell.execute_reply":"2024-10-24T19:21:57.353234Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"list_of_evaluations.append(evaluate_classification_with_model('LSTM', model, X_test_padded, y_test,sentiments))","metadata":{"execution":{"iopub.status.busy":"2024-10-24T19:22:10.313713Z","iopub.execute_input":"2024-10-24T19:22:10.314149Z","iopub.status.idle":"2024-10-24T19:22:13.470553Z","shell.execute_reply.started":"2024-10-24T19:22:10.314114Z","shell.execute_reply":"2024-10-24T19:22:13.469325Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"<a name='4.3'></a>\n\n## GRU","metadata":{}},{"cell_type":"code","source":"embedding_dim = 200\n\n\nstart_time = time.time()\nmodel = Sequential([\n    Embedding(input_dim=len(tokenizer.word_index) + 1, output_dim=embedding_dim, input_length=max_len),\n    GRU(units=100, dropout=0.5, recurrent_dropout=0.5), #return_sequences=True\n    BatchNormalization(momentum=0.9),\n    Dense(units=num_classes, activation='softmax')\n])\nmodel.summary()\n\nmodel.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n\nhistory = model.fit(X_train_padded, y_train, epochs=10, batch_size=128, validation_data=(X_valid_padded, y_valid), callbacks=[early_stopping])\n\nend_time = time.time()\nruntime['GRU'] = end_time - start_time","metadata":{"execution":{"iopub.status.busy":"2024-10-24T19:26:53.120357Z","iopub.execute_input":"2024-10-24T19:26:53.120829Z","iopub.status.idle":"2024-10-24T19:27:52.067059Z","shell.execute_reply.started":"2024-10-24T19:26:53.120796Z","shell.execute_reply":"2024-10-24T19:27:52.065652Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"history_df = pd.DataFrame(history.history)\nplot = plotting_funct(history_df, 'Simple GRU')","metadata":{"execution":{"iopub.status.busy":"2024-10-24T19:28:11.550552Z","iopub.execute_input":"2024-10-24T19:28:11.550952Z","iopub.status.idle":"2024-10-24T19:28:12.142633Z","shell.execute_reply.started":"2024-10-24T19:28:11.550922Z","shell.execute_reply":"2024-10-24T19:28:12.14126Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"y_pred_probs = model.predict(X_test_padded)\ny_pred = np.argmax(y_pred_probs, axis=1)\n\n\nprint('Classification Report for GRU:\\n',classification_report(y_test, y_pred, target_names=sentiments))\n\nconf_matrix(y_test,y_pred,'GRU', sentiments)","metadata":{"execution":{"iopub.status.busy":"2024-10-24T19:29:09.458884Z","iopub.execute_input":"2024-10-24T19:29:09.459424Z","iopub.status.idle":"2024-10-24T19:29:12.931394Z","shell.execute_reply.started":"2024-10-24T19:29:09.459387Z","shell.execute_reply":"2024-10-24T19:29:12.930018Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"list_of_evaluations.append(evaluate_classification_with_model('GRU', model, X_test_padded, y_test,sentiments))","metadata":{"execution":{"iopub.status.busy":"2024-10-24T19:29:22.76141Z","iopub.execute_input":"2024-10-24T19:29:22.76184Z","iopub.status.idle":"2024-10-24T19:29:25.527314Z","shell.execute_reply.started":"2024-10-24T19:29:22.761806Z","shell.execute_reply":"2024-10-24T19:29:25.525811Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"<a name='4.4'></a>\n\n## GRU Using GloVe Embedding","metadata":{}},{"cell_type":"code","source":"# # Split all sentences\nelements = (' '.join([sentence for sentence in X])).split()\n","metadata":{"execution":{"iopub.status.busy":"2024-10-24T19:29:54.135937Z","iopub.execute_input":"2024-10-24T19:29:54.136372Z","iopub.status.idle":"2024-10-24T19:29:54.19613Z","shell.execute_reply.started":"2024-10-24T19:29:54.136339Z","shell.execute_reply":"2024-10-24T19:29:54.194777Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"labels = set(y)","metadata":{"execution":{"iopub.status.busy":"2024-10-24T19:30:00.373176Z","iopub.execute_input":"2024-10-24T19:30:00.37438Z","iopub.status.idle":"2024-10-24T19:30:00.38428Z","shell.execute_reply.started":"2024-10-24T19:30:00.374338Z","shell.execute_reply":"2024-10-24T19:30:00.382615Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def create_lookup_tables(text):\n    \"\"\"Create lookup tables for vocabulary\n    :param text: The text split into words\n    :return: A tuple of dicts (vocab_to_int, int_to_vocab)\n    \"\"\"\n    vocab = set(text)\n    \n    vocab_to_int = {word: i for i, word in enumerate(vocab)}\n    int_to_vocab = {v:k for k, v in vocab_to_int.items()}\n    \n    return vocab_to_int, int_to_vocab","metadata":{"execution":{"iopub.status.busy":"2024-10-24T19:30:06.009343Z","iopub.execute_input":"2024-10-24T19:30:06.009772Z","iopub.status.idle":"2024-10-24T19:30:06.017514Z","shell.execute_reply.started":"2024-10-24T19:30:06.009741Z","shell.execute_reply":"2024-10-24T19:30:06.015811Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"elements.append(\"<UNK>\")\n\n# Map vocabularies to int\nvocab_to_int, int_to_vocab = create_lookup_tables(elements)\nlabels_to_int, int_to_labels = create_lookup_tables(y)\n\nprint(\"Vocabulary of our dataset: {}\".format(len(vocab_to_int)))","metadata":{"execution":{"iopub.status.busy":"2024-10-24T19:30:11.342744Z","iopub.execute_input":"2024-10-24T19:30:11.343181Z","iopub.status.idle":"2024-10-24T19:30:11.427291Z","shell.execute_reply.started":"2024-10-24T19:30:11.34315Z","shell.execute_reply":"2024-10-24T19:30:11.425677Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def convert_to_int(data, data_int):\n    \"\"\"Converts all our text to integers\n    :param data: The text to be converted\n    :return: All sentences in ints\n    \"\"\"\n    all_items = []\n    for sentence in data: \n        all_items.append([data_int[word] if word in data_int else data_int[\"<UNK>\"] for word in sentence.split()])\n    \n    return all_items","metadata":{"execution":{"iopub.status.busy":"2024-10-24T19:30:19.476067Z","iopub.execute_input":"2024-10-24T19:30:19.476582Z","iopub.status.idle":"2024-10-24T19:30:19.483767Z","shell.execute_reply.started":"2024-10-24T19:30:19.476548Z","shell.execute_reply":"2024-10-24T19:30:19.48203Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"enc = OneHotEncoder()\n\nenc.fit(y_train.reshape(-1, 1))\n\n# Fit and transform the target values\ny_train_encoded = enc.fit_transform(y_train.reshape(-1, 1)).toarray()\ny_test_encoded = enc.transform(y_test.values.reshape(-1, 1)).toarray()\ny_valid_encoded = enc.fit_transform(y_valid.reshape(-1, 1)).toarray()","metadata":{"execution":{"iopub.status.busy":"2024-10-24T19:30:25.698573Z","iopub.execute_input":"2024-10-24T19:30:25.699644Z","iopub.status.idle":"2024-10-24T19:30:25.714325Z","shell.execute_reply.started":"2024-10-24T19:30:25.699606Z","shell.execute_reply":"2024-10-24T19:30:25.712964Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Hyperparameters\nmax_sentence_length = 31\n#embedding_vector_length = 300\ndropout = 0.2","metadata":{"execution":{"iopub.status.busy":"2024-10-24T19:30:31.025538Z","iopub.execute_input":"2024-10-24T19:30:31.02597Z","iopub.status.idle":"2024-10-24T19:30:31.031238Z","shell.execute_reply.started":"2024-10-24T19:30:31.025938Z","shell.execute_reply":"2024-10-24T19:30:31.029962Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# the downloaded GloVe path\npath_to_glove_file = 'glove.6B.200d.txt'","metadata":{"execution":{"iopub.status.busy":"2024-10-24T19:30:36.540606Z","iopub.execute_input":"2024-10-24T19:30:36.541143Z","iopub.status.idle":"2024-10-24T19:30:36.547064Z","shell.execute_reply.started":"2024-10-24T19:30:36.541109Z","shell.execute_reply":"2024-10-24T19:30:36.545492Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Importing the glove\nembedding_index = {}\nwith open(path_to_glove_file, encoding = 'utf-8') as f:\n    for line in f:\n        word, coefs = line.split(maxsplit = 1)\n        coefs = np.fromstring(coefs, 'f', sep = ' ')\n        embedding_index[word] = coefs\nprint('Found %s word vectors.' % len(embedding_index))","metadata":{"execution":{"iopub.status.busy":"2024-10-24T19:30:42.690991Z","iopub.execute_input":"2024-10-24T19:30:42.691411Z","iopub.status.idle":"2024-10-24T19:30:42.888586Z","shell.execute_reply.started":"2024-10-24T19:30:42.691382Z","shell.execute_reply":"2024-10-24T19:30:42.88699Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"num_tokens = len(set(elements)) + 2\nembedding_dim = 200\nhits = 0\nmisses = 0","metadata":{"execution":{"iopub.status.busy":"2024-10-24T19:32:52.940905Z","iopub.execute_input":"2024-10-24T19:32:52.941499Z","iopub.status.idle":"2024-10-24T19:32:52.982396Z","shell.execute_reply.started":"2024-10-24T19:32:52.941447Z","shell.execute_reply":"2024-10-24T19:32:52.980745Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Creating the Embedding Matrix\nembedding_matrix = np.zeros((num_tokens, embedding_dim))\nfor word,i in vocab_to_int.items():\n    embedding_vector = embedding_index.get(word)\n    if embedding_vector is not None:\n        embedding_matrix[i] = embedding_vector\n        hits += 1\n    else:\n        misses += 1\nprint('Converted %d words (%d misses)'% (hits, misses))","metadata":{"execution":{"iopub.status.busy":"2024-10-24T19:32:58.910413Z","iopub.execute_input":"2024-10-24T19:32:58.910803Z","iopub.status.idle":"2024-10-24T19:32:58.940595Z","shell.execute_reply.started":"2024-10-24T19:32:58.910773Z","shell.execute_reply":"2024-10-24T19:32:58.939215Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Building the GRU Model\nmodel = Sequential()\nembedding_layer = Embedding(\n    num_tokens,\n    embedding_dim,\n    embeddings_initializer=keras.initializers.constant(embedding_matrix),\n    trainable = False,\n)\nmodel.add(embedding_layer)\nmodel.add(GRU(100, dropout=0.5, recurrent_dropout=0.5, return_sequences=False)) # return_sequences=True,\n#model.add(GRU(100, dropout=dropout, recurrent_dropout=dropout))\nmodel.add(BatchNormalization(momentum=0.9))\nmodel.add(Dense(len(labels), activation='softmax'))","metadata":{"execution":{"iopub.status.busy":"2024-10-24T19:33:05.120761Z","iopub.execute_input":"2024-10-24T19:33:05.121228Z","iopub.status.idle":"2024-10-24T19:33:05.179661Z","shell.execute_reply.started":"2024-10-24T19:33:05.121189Z","shell.execute_reply":"2024-10-24T19:33:05.177839Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Fitting the Model\nstart_time = time.time()\n\noptimizer = keras.optimizers.Adam()\n\nmodel.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['accuracy'])\n\nhistory = model.fit(X_train_padded, y_train_encoded, batch_size=128, epochs=10, validation_data=(X_valid_padded, y_valid_encoded), callbacks=[early_stopping])\n\nend_time = time.time()\nruntime['GRU with GloVe'] = end_time - start_time","metadata":{"execution":{"iopub.status.busy":"2024-10-24T19:33:11.874488Z","iopub.execute_input":"2024-10-24T19:33:11.874881Z","iopub.status.idle":"2024-10-24T19:33:11.926751Z","shell.execute_reply.started":"2024-10-24T19:33:11.874851Z","shell.execute_reply":"2024-10-24T19:33:11.925112Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"history_df = pd.DataFrame(history.history)\nplot = plotting_funct(history_df, 'GRU with GloVe Embedding')","metadata":{"execution":{"iopub.status.busy":"2024-10-24T19:33:20.143147Z","iopub.execute_input":"2024-10-24T19:33:20.143606Z","iopub.status.idle":"2024-10-24T19:33:20.689048Z","shell.execute_reply.started":"2024-10-24T19:33:20.14357Z","shell.execute_reply":"2024-10-24T19:33:20.687816Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"int_to_labels = {4: 'religion', 0: 'age', 1: 'ethnicity', 2: 'gender', 3: 'not_cyberbullying'}\n\n# Map predictions to categories\ny_pred_categories = [int_to_labels[pred] for pred in y_pred]","metadata":{"execution":{"iopub.status.busy":"2024-10-24T19:33:29.743737Z","iopub.execute_input":"2024-10-24T19:33:29.744144Z","iopub.status.idle":"2024-10-24T19:33:29.752276Z","shell.execute_reply.started":"2024-10-24T19:33:29.744113Z","shell.execute_reply":"2024-10-24T19:33:29.750838Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"y_test_numerical = np.argmax(y_test_encoded, axis=1)","metadata":{"execution":{"iopub.status.busy":"2024-10-24T19:33:35.151105Z","iopub.execute_input":"2024-10-24T19:33:35.152186Z","iopub.status.idle":"2024-10-24T19:33:35.157431Z","shell.execute_reply.started":"2024-10-24T19:33:35.152147Z","shell.execute_reply":"2024-10-24T19:33:35.156065Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"y_pred_probs = model.predict(X_test_padded)\ny_pred = np.argmax(y_pred_probs, axis=1)\n\nprint('Classification Report for GRU with Glove Embedding:\\n',classification_report(y_pred, y_test))\nsentiments2 = [\"age\",\"ethnicity\",\"gender\",\"not bullying\",\"religion\"]\nconf_matrix(y_pred, y_test,'GRU with Glove Embedding',sentiments)","metadata":{"execution":{"iopub.status.busy":"2024-10-24T19:33:41.245885Z","iopub.execute_input":"2024-10-24T19:33:41.246295Z","iopub.status.idle":"2024-10-24T19:33:41.429114Z","shell.execute_reply.started":"2024-10-24T19:33:41.246262Z","shell.execute_reply":"2024-10-24T19:33:41.427534Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Final evaluation of the model\nscores = model.evaluate(X_test_padded, y_test_encoded, verbose=0)\nprint(\"Accuracy: %.2f%%\" % (scores[1]*100))","metadata":{"execution":{"iopub.status.busy":"2024-10-24T19:33:50.057063Z","iopub.execute_input":"2024-10-24T19:33:50.05841Z","iopub.status.idle":"2024-10-24T19:33:50.156663Z","shell.execute_reply.started":"2024-10-24T19:33:50.058364Z","shell.execute_reply":"2024-10-24T19:33:50.155109Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"list_of_evaluations.append(evaluate_classification_with_model('GRU with Glove', model, X_test_padded, y_test,sentiments))","metadata":{"execution":{"iopub.status.busy":"2024-10-24T19:33:55.246649Z","iopub.execute_input":"2024-10-24T19:33:55.247794Z","iopub.status.idle":"2024-10-24T19:33:55.44682Z","shell.execute_reply.started":"2024-10-24T19:33:55.247744Z","shell.execute_reply":"2024-10-24T19:33:55.444779Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Attention Is All You Need\nIn this section, it is tried to deploy the GRU model with a Word2Vec embedding matrix and attention mechanism using the PyTorch library.\n\n\nThe code for this section has been obtained from: [Kaggle](https://www.kaggle.com/code/ludovicocuoghi/detecting-bullying-tweets-pytorch-lstm-bert)\n","metadata":{}},{"cell_type":"markdown","source":"<a name='4.5'></a>\n\n## GRU (Word2Vec & Attention)","metadata":{}},{"cell_type":"code","source":"# With the help of This function tweets are tokenized \ndef Tokenize(column, seq_len):\n    ##Create vocabulary of words from column\n    corpus = [word for text in column for word in text.split()]\n    count_words = Counter(corpus)\n    sorted_words = count_words.most_common()\n    vocab_to_int = {w:i+1 for i, (w,c) in enumerate(sorted_words)}\n\n    ##Tokenize the columns text using the vocabulary\n    text_int = []\n    for text in column:\n        r = [vocab_to_int[word] for word in text.split()]\n        text_int.append(r)\n    ##Add padding to tokens\n    features = np.zeros((len(text_int), seq_len), dtype = int)\n    for i, review in enumerate(text_int):\n        if len(review) <= seq_len:\n            zeros = list(np.zeros(seq_len - len(review)))\n            new = zeros + review\n        else:\n            new = review[: seq_len]\n        features[i, :] = np.array(new)\n\n    return sorted_words, features","metadata":{"execution":{"iopub.status.busy":"2024-10-24T19:35:29.239708Z","iopub.execute_input":"2024-10-24T19:35:29.240206Z","iopub.status.idle":"2024-10-24T19:35:29.250341Z","shell.execute_reply.started":"2024-10-24T19:35:29.24017Z","shell.execute_reply":"2024-10-24T19:35:29.248857Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"vocabulary, tokenized_column = Tokenize(df[\"text_clean\"], max_len)","metadata":{"execution":{"iopub.status.busy":"2024-10-24T19:35:38.870669Z","iopub.execute_input":"2024-10-24T19:35:38.871097Z","iopub.status.idle":"2024-10-24T19:35:39.504385Z","shell.execute_reply.started":"2024-10-24T19:35:38.871063Z","shell.execute_reply":"2024-10-24T19:35:39.503082Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"keys = []\nvalues = []\nfor key, value in vocabulary[:20]:\n    keys.append(key)\n    values.append(value)","metadata":{"execution":{"iopub.status.busy":"2024-10-24T19:35:44.179024Z","iopub.execute_input":"2024-10-24T19:35:44.179464Z","iopub.status.idle":"2024-10-24T19:35:44.185824Z","shell.execute_reply.started":"2024-10-24T19:35:44.179431Z","shell.execute_reply":"2024-10-24T19:35:44.184547Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Word Embedding by Word2Vec","metadata":{}},{"cell_type":"code","source":"Word2vec_train_data = list(map(lambda x: x.split(), X_train))","metadata":{"execution":{"iopub.status.busy":"2024-10-24T19:35:55.64432Z","iopub.execute_input":"2024-10-24T19:35:55.644742Z","iopub.status.idle":"2024-10-24T19:35:55.693087Z","shell.execute_reply.started":"2024-10-24T19:35:55.644708Z","shell.execute_reply":"2024-10-24T19:35:55.691912Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"EMBEDDING_DIM = 200","metadata":{"execution":{"iopub.status.busy":"2024-10-24T19:36:00.436388Z","iopub.execute_input":"2024-10-24T19:36:00.436819Z","iopub.status.idle":"2024-10-24T19:36:00.442562Z","shell.execute_reply.started":"2024-10-24T19:36:00.436785Z","shell.execute_reply":"2024-10-24T19:36:00.440975Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"word2vec_model = Word2Vec(Word2vec_train_data, vector_size=EMBEDDING_DIM)","metadata":{"execution":{"iopub.status.busy":"2024-10-24T19:36:04.137516Z","iopub.execute_input":"2024-10-24T19:36:04.13922Z","iopub.status.idle":"2024-10-24T19:36:06.07634Z","shell.execute_reply.started":"2024-10-24T19:36:04.139118Z","shell.execute_reply":"2024-10-24T19:36:06.074595Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(f\"Vocabulary size: {len(vocabulary) + 1}\")","metadata":{"execution":{"iopub.status.busy":"2024-10-24T19:36:08.512921Z","iopub.execute_input":"2024-10-24T19:36:08.513361Z","iopub.status.idle":"2024-10-24T19:36:08.519602Z","shell.execute_reply.started":"2024-10-24T19:36:08.513329Z","shell.execute_reply":"2024-10-24T19:36:08.518172Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"VOCAB_SIZE = len(vocabulary) + 1 #+1 for the padding","metadata":{"execution":{"iopub.status.busy":"2024-10-24T19:36:12.316428Z","iopub.execute_input":"2024-10-24T19:36:12.316848Z","iopub.status.idle":"2024-10-24T19:36:12.322518Z","shell.execute_reply.started":"2024-10-24T19:36:12.316812Z","shell.execute_reply":"2024-10-24T19:36:12.321228Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Define an empty embedding matrix of shape (VOCAB_SIZE, EMBEDDING_DIM)\nembedding_matrix = np.zeros((VOCAB_SIZE, EMBEDDING_DIM))\n\n# Fill the embedding matrix with pre-trained values from word2vec\nfor word, token in vocabulary:\n    # Check if the word is present in the word2vec model's vocabulary\n    if word in word2vec_model.wv.key_to_index:\n        # If the word is present, retrieve its embedding vector and add it to the embedding matrix\n        embedding_vector = word2vec_model.wv[word]\n        embedding_matrix[token] = embedding_vector\n\n# Print the shape of the embedding matrix\nprint(\"Embedding Matrix Shape:\", embedding_matrix.shape)","metadata":{"execution":{"iopub.status.busy":"2024-10-24T19:36:17.017183Z","iopub.execute_input":"2024-10-24T19:36:17.017691Z","iopub.status.idle":"2024-10-24T19:36:17.068535Z","shell.execute_reply.started":"2024-10-24T19:36:17.017656Z","shell.execute_reply":"2024-10-24T19:36:17.067162Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Train - Validation - Test split","metadata":{}},{"cell_type":"code","source":"X = tokenized_column\ny = df['sentiment_code'].values","metadata":{"execution":{"iopub.status.busy":"2024-10-24T19:36:29.948058Z","iopub.execute_input":"2024-10-24T19:36:29.949478Z","iopub.status.idle":"2024-10-24T19:36:29.95452Z","shell.execute_reply.started":"2024-10-24T19:36:29.949436Z","shell.execute_reply":"2024-10-24T19:36:29.953154Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, stratify=y, random_state=seed_value)","metadata":{"execution":{"iopub.status.busy":"2024-10-24T19:36:59.25649Z","iopub.execute_input":"2024-10-24T19:36:59.256932Z","iopub.status.idle":"2024-10-24T19:36:59.280588Z","shell.execute_reply.started":"2024-10-24T19:36:59.256895Z","shell.execute_reply":"2024-10-24T19:36:59.279269Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"X_train, X_valid, y_train, y_valid = train_test_split(X_train, y_train, test_size=0.2, stratify=y_train, random_state=seed_value)","metadata":{"execution":{"iopub.status.busy":"2024-10-24T19:37:14.667665Z","iopub.execute_input":"2024-10-24T19:37:14.668089Z","iopub.status.idle":"2024-10-24T19:37:14.688559Z","shell.execute_reply.started":"2024-10-24T19:37:14.668057Z","shell.execute_reply":"2024-10-24T19:37:14.6873Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"(unique, counts) = np.unique(y_train, return_counts=True)\nnp.asarray((unique, counts)).T","metadata":{"execution":{"iopub.status.busy":"2024-10-24T19:37:20.460559Z","iopub.execute_input":"2024-10-24T19:37:20.460983Z","iopub.status.idle":"2024-10-24T19:37:20.470713Z","shell.execute_reply.started":"2024-10-24T19:37:20.460951Z","shell.execute_reply":"2024-10-24T19:37:20.469353Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"ros = RandomOverSampler()\nX_train_os, y_train_os = ros.fit_resample(np.array(X_train),np.array(y_train))","metadata":{"execution":{"iopub.status.busy":"2024-10-24T19:37:27.062855Z","iopub.execute_input":"2024-10-24T19:37:27.063329Z","iopub.status.idle":"2024-10-24T19:37:27.079455Z","shell.execute_reply.started":"2024-10-24T19:37:27.063293Z","shell.execute_reply":"2024-10-24T19:37:27.077877Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"(unique, counts) = np.unique(y_train_os, return_counts=True)\nnp.asarray((unique, counts)).T","metadata":{"execution":{"iopub.status.busy":"2024-10-24T19:37:35.003683Z","iopub.execute_input":"2024-10-24T19:37:35.004123Z","iopub.status.idle":"2024-10-24T19:37:35.013077Z","shell.execute_reply.started":"2024-10-24T19:37:35.00409Z","shell.execute_reply":"2024-10-24T19:37:35.01185Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### PyTorch datasets and dataloaders","metadata":{}},{"cell_type":"code","source":"train_data = TensorDataset(torch.from_numpy(X_train), torch.from_numpy(y_train))\ntest_data = TensorDataset(torch.from_numpy(X_test), torch.from_numpy(y_test))\nvalid_data = TensorDataset(torch.from_numpy(X_valid), torch.from_numpy(y_valid))","metadata":{"execution":{"iopub.status.busy":"2024-10-24T19:37:47.286507Z","iopub.execute_input":"2024-10-24T19:37:47.287038Z","iopub.status.idle":"2024-10-24T19:37:47.304833Z","shell.execute_reply.started":"2024-10-24T19:37:47.287004Z","shell.execute_reply":"2024-10-24T19:37:47.303482Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"BATCH_SIZE = 32","metadata":{"execution":{"iopub.status.busy":"2024-10-24T19:37:51.661995Z","iopub.execute_input":"2024-10-24T19:37:51.662395Z","iopub.status.idle":"2024-10-24T19:37:51.667855Z","shell.execute_reply.started":"2024-10-24T19:37:51.662365Z","shell.execute_reply":"2024-10-24T19:37:51.666333Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train_loader = DataLoader(train_data, shuffle=True, batch_size=BATCH_SIZE, drop_last=True) \nvalid_loader = DataLoader(valid_data, shuffle=False, batch_size=BATCH_SIZE, drop_last=True)\ntest_loader = DataLoader(test_data, shuffle=False, batch_size=BATCH_SIZE, drop_last=True)","metadata":{"execution":{"iopub.status.busy":"2024-10-24T19:37:55.923756Z","iopub.execute_input":"2024-10-24T19:37:55.924178Z","iopub.status.idle":"2024-10-24T19:37:55.93069Z","shell.execute_reply.started":"2024-10-24T19:37:55.924145Z","shell.execute_reply":"2024-10-24T19:37:55.92922Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### PyTorch GRU modeling with Attention Layer","metadata":{}},{"cell_type":"code","source":"class Attention(nn.Module):\n    def __init__(self, hidden_dim, is_bidirectional):\n        super(Attention, self).__init__()\n        self.is_bidirectional = is_bidirectional\n        # The attention linear layer which transforms the input data to the hidden space\n        self.attn = nn.Linear(hidden_dim * (4 if is_bidirectional else 2), hidden_dim * (2 if is_bidirectional else 1))\n        # The linear layer that calculates the attention scores\n        self.v = nn.Linear(hidden_dim * (2 if is_bidirectional else 1), 1, bias=False)\n\n    def forward(self, hidden, encoder_outputs):\n        seq_len = encoder_outputs.size(1)\n        # Concatenate the last two hidden states in case of a bidirectional LSTM\n        if self.is_bidirectional:\n            hidden = torch.cat((hidden[-2], hidden[-1]), dim=-1)\n        else:\n            hidden = hidden[-1]\n        # Repeat the hidden state across the sequence length\n        hidden_repeated = hidden.unsqueeze(1).repeat(1, seq_len, 1)\n        # Calculate attention weights\n        attn_weights = torch.tanh(self.attn(torch.cat((hidden_repeated, encoder_outputs), dim=2)))\n        # Compute attention scores\n        attn_weights = self.v(attn_weights).squeeze(2)\n        # Apply softmax to get valid probabilities\n        return nn.functional.softmax(attn_weights, dim=1)","metadata":{"execution":{"iopub.status.busy":"2024-10-24T19:38:10.852718Z","iopub.execute_input":"2024-10-24T19:38:10.853125Z","iopub.status.idle":"2024-10-24T19:38:10.869537Z","shell.execute_reply.started":"2024-10-24T19:38:10.853095Z","shell.execute_reply":"2024-10-24T19:38:10.868082Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class GRU_Sentiment_Classifier(nn.Module):\n    def __init__(self, vocab_size, embedding_dim, hidden_dim, num_classes, gru_layers, dropout, is_bidirectional):\n        super(GRU_Sentiment_Classifier, self).__init__()\n        self.hidden_dim = hidden_dim\n        self.num_layers = gru_layers\n        self.is_bidirectional = is_bidirectional\n\n        # The Embedding layer that converts input words to embeddings\n        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n        # GRU layer which processes the embeddings\n        self.gru = nn.GRU(embedding_dim, hidden_dim, gru_layers, batch_first=True, bidirectional=is_bidirectional)\n        # Attention layer to compute the context vector\n        self.attention = Attention(hidden_dim, is_bidirectional)\n        # Fully connected layer which classifies the context vector into classes\n        self.fc = nn.Linear(hidden_dim * (2 if is_bidirectional else 1), num_classes)\n        # Apply LogSoftmax to outputs for numerical stability\n        self.softmax = nn.LogSoftmax(dim=1)\n        # Dropout layer for regularisation\n        self.dropout = nn.Dropout(dropout)\n\n    def forward(self, x, hidden):\n        # Transform words to embeddings\n        embedded = self.embedding(x)\n        # Pass embeddings to GRU\n        out, hidden = self.gru(embedded, hidden)\n        # Calculate attention weights\n        attn_weights = self.attention(hidden, out)\n        # Calculate context vector by taking the weighted sum of GRU outputs\n        context = attn_weights.unsqueeze(1).bmm(out).squeeze(1)\n        # Classify the context vector\n        out = self.softmax(self.fc(context))\n        return out, hidden\n\n    def init_hidden(self, batch_size):\n        # Factor determines the size of hidden states depending on bidirectionality\n        factor = 2 if self.is_bidirectional else 1\n        # Initial hidden state is zero\n        h0 = torch.zeros(self.num_layers * factor, batch_size, self.hidden_dim).to(DEVICE)\n        return h0\n","metadata":{"execution":{"iopub.status.busy":"2024-10-24T19:38:24.262374Z","iopub.execute_input":"2024-10-24T19:38:24.262827Z","iopub.status.idle":"2024-10-24T19:38:24.274614Z","shell.execute_reply.started":"2024-10-24T19:38:24.262791Z","shell.execute_reply":"2024-10-24T19:38:24.273327Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"NUM_CLASSES = 5  # We are dealing with a multiclass classification of 5 classes\nHIDDEN_DIM = 100  # Number of neurons of the internal state (internal neural network in the LSTM)\nGRU_LAYERS = 1  # Number of stacked GRU layers\n\nIS_BIDIRECTIONAL = False  # Set this to False for unidirectional GRU, and True for bidirectional GRU\n\nLR = 4e-4  # Learning rate\nDROPOUT = 0.5  # GRU Dropout\nEPOCHS = 10  # Number of training epochs\n\nDEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n\nmodel = GRU_Sentiment_Classifier(VOCAB_SIZE, EMBEDDING_DIM, HIDDEN_DIM, NUM_CLASSES, GRU_LAYERS, DROPOUT, IS_BIDIRECTIONAL)\n\nmodel = model.to(DEVICE)\n\n# Initialize the embedding layer with the previously defined embedding matrix\nmodel.embedding.weight.data.copy_(torch.from_numpy(embedding_matrix))\n# Allow the embedding matrix to be fine-tuned to better adapt to our dataset and get higher accuracy\nmodel.embedding.weight.requires_grad = True\n\n# Set up the criterion (loss function)\ncriterion = nn.NLLLoss()\noptimizer = torch.optim.AdamW(model.parameters(), lr=LR, weight_decay=5e-6)\n\nprint(model)\n","metadata":{"execution":{"iopub.status.busy":"2024-10-24T19:38:45.383123Z","iopub.execute_input":"2024-10-24T19:38:45.383581Z","iopub.status.idle":"2024-10-24T19:38:46.184471Z","shell.execute_reply.started":"2024-10-24T19:38:45.383547Z","shell.execute_reply":"2024-10-24T19:38:46.183167Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"total_step = len(train_loader)\ntotal_step_val = len(valid_loader)\n\nearly_stopping_patience = 4\nearly_stopping_counter = 0\n\nstart_time = time.time()\n\nvalid_acc_max = 0  # Initialize best accuracy top 0\n\nfor e in range(EPOCHS):\n\n    # Lists to host the train and validation losses of every batch for each epoch\n    train_loss, valid_loss = [], []\n    # Lists to host the train and validation accuracy of every batch for each epoch\n    train_acc, valid_acc = [], []\n\n    # Lists to host the train and validation predictions of every batch for each epoch\n    y_train_list, y_val_list = [], []\n\n    # Initalize number of total and correctly classified texts during training and validation\n    correct, correct_val = 0, 0\n    total, total_val = 0, 0\n    running_loss, running_loss_val = 0, 0\n\n    ####TRAINING LOOP####\n    model.train()\n\n    for inputs, labels in train_loader:\n        inputs, labels = inputs.to(DEVICE), labels.to(DEVICE)  # Load features and targets in device\n\n        h = model.init_hidden(labels.size(0))\n\n        model.zero_grad()  # Reset gradients\n\n        output, h = model(inputs, h)  # Get output and hidden states from GRU network\n\n        loss = criterion(output, labels)\n        loss.backward()\n\n        running_loss += loss.item()\n\n        optimizer.step()\n\n        y_pred_train = torch.argmax(output, dim=1)  # Get tensor of predicted values on the training set\n        y_train_list.extend(y_pred_train.squeeze().tolist())  # Transform tensor to list and the values to the list\n\n        correct += torch.sum(y_pred_train == labels).item()  # Count correctly classified texts per batch\n        total += labels.size(0)  # Count total texts per batch\n\n    train_loss.append(running_loss / total_step)\n    train_acc.append(100 * correct / total)\n\n    ####VALIDATION LOOP####\n    with torch.no_grad():\n\n        model.eval()\n\n        for inputs, labels in valid_loader:\n            inputs, labels = inputs.to(DEVICE), labels.to(DEVICE)\n\n            val_h = model.init_hidden(labels.size(0))\n\n            output, val_h = model(inputs, val_h)\n\n            val_loss = criterion(output, labels)\n            running_loss_val += val_loss.item()\n\n            y_pred_val = torch.argmax(output, dim=1)\n            y_val_list.extend(y_pred_val.squeeze().tolist())\n\n            correct_val += torch.sum(y_pred_val == labels).item()\n            total_val += labels.size(0)\n\n        valid_loss.append(running_loss_val / total_step_val)\n        valid_acc.append(100 * correct_val / total_val)\n\n    # Save model if validation accuracy increases\n    if np.mean(valid_acc) >= valid_acc_max:\n        torch.save(model.state_dict(), './state_dict.pt')\n        print(f'Epoch {e+1}:Validation accuracy increased ({valid_acc_max:.6f} --> {np.mean(valid_acc):.6f}).  Saving model ...')\n        valid_acc_max = np.mean(valid_acc)\n        early_stopping_counter = 0  # Reset counter if validation accuracy increases\n    else:\n        print(f'Epoch {e+1}:Validation accuracy did not increase')\n        early_stopping_counter += 1  # Increase counter if validation accuracy does not increase\n\n    if early_stopping_counter > early_stopping_patience:\n        print('Early stopped at epoch :', e+1)\n        break\n\n    print(f'\\tTrain_loss : {np.mean(train_loss):.4f} Val_loss : {np.mean(valid_loss):.4f}')\n    print(f'\\tTrain_acc : {np.mean(train_acc):.3f}% Val_acc : {np.mean(valid_acc):.3f}%')\n\nend_time = time.time()\nruntime['GRU with Word2Vec & Attention'] = end_time - start_time","metadata":{"execution":{"iopub.status.busy":"2024-10-24T19:38:55.627413Z","iopub.execute_input":"2024-10-24T19:38:55.628495Z","iopub.status.idle":"2024-10-24T19:46:38.695578Z","shell.execute_reply.started":"2024-10-24T19:38:55.628457Z","shell.execute_reply":"2024-10-24T19:46:38.694179Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Loading the best model\nmodel.load_state_dict(torch.load('./state_dict.pt'))","metadata":{"execution":{"iopub.status.busy":"2024-10-24T19:46:42.544359Z","iopub.execute_input":"2024-10-24T19:46:42.5448Z","iopub.status.idle":"2024-10-24T19:46:42.581045Z","shell.execute_reply.started":"2024-10-24T19:46:42.544764Z","shell.execute_reply":"2024-10-24T19:46:42.579838Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def evaluate_model(model, test_loader):\n    model.eval()\n    y_pred_list = []\n    y_test_list = []\n    \n    with torch.no_grad():\n        for inputs, labels in test_loader:\n            inputs, labels = inputs.to(DEVICE), labels.to(DEVICE)\n            test_h = model.init_hidden(labels.size(0))\n\n            output, val_h = model(inputs, test_h)\n            y_pred_test = torch.argmax(output, dim=1)\n            y_pred_list.extend(y_pred_test.squeeze().tolist())\n            y_test_list.extend(labels.squeeze().tolist())\n    \n    return y_pred_list, y_test_list\n\ny_pred_list, y_test_list = evaluate_model(model, test_loader)","metadata":{"execution":{"iopub.status.busy":"2024-10-24T19:46:50.276696Z","iopub.execute_input":"2024-10-24T19:46:50.277132Z","iopub.status.idle":"2024-10-24T19:46:51.685287Z","shell.execute_reply.started":"2024-10-24T19:46:50.277093Z","shell.execute_reply":"2024-10-24T19:46:51.68399Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print('Classification Report for GRU :\\n', classification_report(y_test_list, y_pred_list, target_names=sentiments))","metadata":{"execution":{"iopub.status.busy":"2024-10-24T19:47:02.313635Z","iopub.execute_input":"2024-10-24T19:47:02.314044Z","iopub.status.idle":"2024-10-24T19:47:02.349679Z","shell.execute_reply.started":"2024-10-24T19:47:02.314014Z","shell.execute_reply":"2024-10-24T19:47:02.347467Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"conf_matrix(y_test_list,y_pred_list,'GRU with Word2Vec embedding and attention layer', sentiments)","metadata":{"execution":{"iopub.status.busy":"2024-10-24T19:47:13.428395Z","iopub.execute_input":"2024-10-24T19:47:13.428815Z","iopub.status.idle":"2024-10-24T19:47:13.790033Z","shell.execute_reply.started":"2024-10-24T19:47:13.428781Z","shell.execute_reply":"2024-10-24T19:47:13.78877Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# saving the evaluation metrics for GRU (Word2Vec and Attention)\nevaluation = {}\nevaluation['Model'] = 'GRU (Word2Veb & Attention)'\nevaluation['Accuracy'] = accuracy_score(y_test_list, y_pred_list)\nevaluation['Precision'] = precision_score(y_test_list, y_pred_list, average='macro')\nevaluation['Recall'] = recall_score(y_test_list, y_pred_list, average='macro')\nevaluation['F1-score'] = f1_score(y_test_list, y_pred_list, average='macro')\n\nlist_of_evaluations.append(evaluation)","metadata":{"execution":{"iopub.status.busy":"2024-10-24T19:47:25.021406Z","iopub.execute_input":"2024-10-24T19:47:25.021827Z","iopub.status.idle":"2024-10-24T19:47:25.063618Z","shell.execute_reply.started":"2024-10-24T19:47:25.021792Z","shell.execute_reply":"2024-10-24T19:47:25.062394Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"<a name='4.6'></a>\n## Bert \n\nIn this section, an attempt is made to utilize the pre-trained BERT classifier.\n\n\n\n\nThe code for this section has been obtained from: [Kaggle](https://www.kaggle.com/code/ludovicocuoghi/detecting-bullying-tweets-pytorch-lstm-bert)\n","metadata":{}},{"cell_type":"code","source":"X = df['text_clean'].values\ny = df['sentiment_code'].values","metadata":{"execution":{"iopub.status.busy":"2024-10-24T19:47:43.508046Z","iopub.execute_input":"2024-10-24T19:47:43.508475Z","iopub.status.idle":"2024-10-24T19:47:43.514516Z","shell.execute_reply.started":"2024-10-24T19:47:43.508441Z","shell.execute_reply":"2024-10-24T19:47:43.513161Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, stratify=y, random_state=seed_value)","metadata":{"execution":{"iopub.status.busy":"2024-10-24T19:47:48.180789Z","iopub.execute_input":"2024-10-24T19:47:48.18133Z","iopub.status.idle":"2024-10-24T19:47:48.209964Z","shell.execute_reply.started":"2024-10-24T19:47:48.181287Z","shell.execute_reply":"2024-10-24T19:47:48.20862Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"X_train, X_valid, y_train, y_valid = train_test_split(X_train, y_train, test_size=0.2, stratify=y_train, random_state=seed_value)","metadata":{"execution":{"iopub.status.busy":"2024-10-24T19:48:53.673732Z","iopub.execute_input":"2024-10-24T19:48:53.674202Z","iopub.status.idle":"2024-10-24T19:48:53.69526Z","shell.execute_reply.started":"2024-10-24T19:48:53.67417Z","shell.execute_reply":"2024-10-24T19:48:53.693985Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"ros = RandomOverSampler()\nX_train_os, y_train_os = ros.fit_resample(np.array(X_train).reshape(-1,1),np.array(y_train).reshape(-1,1))","metadata":{"execution":{"iopub.status.busy":"2024-10-24T19:48:58.020368Z","iopub.execute_input":"2024-10-24T19:48:58.020788Z","iopub.status.idle":"2024-10-24T19:48:58.03581Z","shell.execute_reply.started":"2024-10-24T19:48:58.020755Z","shell.execute_reply":"2024-10-24T19:48:58.034494Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"X_train_os = X_train_os.flatten()\ny_train_os = y_train_os.flatten()","metadata":{"execution":{"iopub.status.busy":"2024-10-24T19:49:03.860869Z","iopub.execute_input":"2024-10-24T19:49:03.861346Z","iopub.status.idle":"2024-10-24T19:49:03.86832Z","shell.execute_reply.started":"2024-10-24T19:49:03.86131Z","shell.execute_reply":"2024-10-24T19:49:03.867013Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"(unique, counts) = np.unique(y_train_os, return_counts=True)\nnp.asarray((unique, counts)).T","metadata":{"execution":{"iopub.status.busy":"2024-10-24T19:49:08.469567Z","iopub.execute_input":"2024-10-24T19:49:08.470085Z","iopub.status.idle":"2024-10-24T19:49:08.480363Z","shell.execute_reply.started":"2024-10-24T19:49:08.470051Z","shell.execute_reply":"2024-10-24T19:49:08.479114Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### BERT Tokenization","metadata":{}},{"cell_type":"code","source":"tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)","metadata":{"execution":{"iopub.status.busy":"2024-10-24T19:49:22.6469Z","iopub.execute_input":"2024-10-24T19:49:22.647344Z","iopub.status.idle":"2024-10-24T19:49:24.529298Z","shell.execute_reply.started":"2024-10-24T19:49:22.647311Z","shell.execute_reply":"2024-10-24T19:49:24.528197Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def bert_tokenizer(data):\n    input_ids = []\n    attention_masks = []\n    for sent in data:\n        encoded_sent = tokenizer.encode_plus(\n            text=sent,\n            add_special_tokens=True,        # Add `[CLS]` and `[SEP]` special tokens\n            max_length=MAX_LEN,             # Choose max length to truncate/pad\n            pad_to_max_length=True,         # Pad sentence to max length \n            return_attention_mask=True      # Return attention mask\n            )\n        input_ids.append(encoded_sent.get('input_ids'))\n        attention_masks.append(encoded_sent.get('attention_mask'))\n\n    # Convert lists to tensors\n    input_ids = torch.tensor(input_ids)\n    attention_masks = torch.tensor(attention_masks)\n\n    return input_ids, attention_masks","metadata":{"execution":{"iopub.status.busy":"2024-10-24T19:49:41.027618Z","iopub.execute_input":"2024-10-24T19:49:41.028177Z","iopub.status.idle":"2024-10-24T19:49:41.035927Z","shell.execute_reply.started":"2024-10-24T19:49:41.028142Z","shell.execute_reply":"2024-10-24T19:49:41.034341Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Tokenize train tweets\nencoded_tweets = [tokenizer.encode(sent, add_special_tokens=True) for sent in X_train]\n\n# Find the longest tokenized tweet\nmax_len = max([len(sent) for sent in encoded_tweets])\nprint('Max length: ', max_len)","metadata":{"execution":{"iopub.status.busy":"2024-10-24T19:49:50.431343Z","iopub.execute_input":"2024-10-24T19:49:50.43178Z","iopub.status.idle":"2024-10-24T19:50:03.786759Z","shell.execute_reply.started":"2024-10-24T19:49:50.431745Z","shell.execute_reply":"2024-10-24T19:50:03.785336Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"MAX_LEN = 128","metadata":{"execution":{"iopub.status.busy":"2024-10-24T19:50:03.788971Z","iopub.execute_input":"2024-10-24T19:50:03.789376Z","iopub.status.idle":"2024-10-24T19:50:03.794832Z","shell.execute_reply.started":"2024-10-24T19:50:03.789343Z","shell.execute_reply":"2024-10-24T19:50:03.793609Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train_inputs, train_masks = bert_tokenizer(X_train_os)\nval_inputs, val_masks = bert_tokenizer(X_valid)\ntest_inputs, test_masks = bert_tokenizer(X_test)","metadata":{"execution":{"iopub.status.busy":"2024-10-24T19:50:11.717873Z","iopub.execute_input":"2024-10-24T19:50:11.718285Z","iopub.status.idle":"2024-10-24T19:50:35.864116Z","shell.execute_reply.started":"2024-10-24T19:50:11.718237Z","shell.execute_reply":"2024-10-24T19:50:35.862711Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Data preprocessing for PyTorch BERT model","metadata":{}},{"cell_type":"code","source":"# Convert target columns to pytorch tensors format\ntrain_labels = torch.from_numpy(y_train_os)\nval_labels = torch.from_numpy(y_valid)\ntest_labels = torch.from_numpy(y_test)","metadata":{"execution":{"iopub.status.busy":"2024-10-24T19:50:35.866473Z","iopub.execute_input":"2024-10-24T19:50:35.866935Z","iopub.status.idle":"2024-10-24T19:50:35.873417Z","shell.execute_reply.started":"2024-10-24T19:50:35.866895Z","shell.execute_reply":"2024-10-24T19:50:35.872016Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Dataloaders","metadata":{}},{"cell_type":"code","source":"batch_size = 32","metadata":{"execution":{"iopub.status.busy":"2024-10-24T19:50:37.669942Z","iopub.execute_input":"2024-10-24T19:50:37.670397Z","iopub.status.idle":"2024-10-24T19:50:37.676542Z","shell.execute_reply.started":"2024-10-24T19:50:37.670363Z","shell.execute_reply":"2024-10-24T19:50:37.674616Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Create the DataLoader for our training set\ntrain_data = TensorDataset(train_inputs, train_masks, train_labels)\ntrain_sampler = RandomSampler(train_data)\ntrain_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)\n\n# Create the DataLoader for our validation set\nval_data = TensorDataset(val_inputs, val_masks, val_labels)\nval_sampler = SequentialSampler(val_data)\nval_dataloader = DataLoader(val_data, sampler=val_sampler, batch_size=batch_size)\n\n# Create the DataLoader for our test set\ntest_data = TensorDataset(test_inputs, test_masks, test_labels)\ntest_sampler = SequentialSampler(test_data)\ntest_dataloader = DataLoader(test_data, sampler=test_sampler, batch_size=batch_size)","metadata":{"execution":{"iopub.status.busy":"2024-10-24T19:50:42.792481Z","iopub.execute_input":"2024-10-24T19:50:42.792935Z","iopub.status.idle":"2024-10-24T19:50:42.800784Z","shell.execute_reply.started":"2024-10-24T19:50:42.792901Z","shell.execute_reply":"2024-10-24T19:50:42.799228Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### BERT Modeling","metadata":{}},{"cell_type":"code","source":"class Bert_Classifier(nn.Module):\n    def __init__(self, freeze_bert=False):\n        super(Bert_Classifier, self).__init__()\n        # Specify hidden size of BERT, hidden size of the classifier, and number of labels\n        n_input = 768\n        n_hidden = 50\n        n_output = 5\n\n        # Instantiate BERT model\n        self.bert = BertModel.from_pretrained('bert-base-uncased')\n\n        # Instantiate the classifier (a fully connected layer followed by a ReLU activation and another fully connected layer)\n        self.classifier = nn.Sequential(\n            nn.Linear(n_input, n_hidden),\n            nn.ReLU(),\n            nn.Linear(n_hidden, n_output)\n        )\n\n        # Freeze the BERT model weights if freeze_bert is True (useful for feature extraction without fine-tuning)\n        if freeze_bert:\n            for param in self.bert.parameters():\n                param.requires_grad = False\n\n    def forward(self, input_ids, attention_mask):\n        # Feed input data (input_ids and attention_mask) to BERT\n        outputs = self.bert(input_ids=input_ids,\n                            attention_mask=attention_mask)\n\n        # Extract the last hidden state of the `[CLS]` token from the BERT output (useful for classification tasks)\n        last_hidden_state_cls = outputs[0][:, 0, :]\n\n        # Feed the extracted hidden state to the classifier to compute logits\n        logits = self.classifier(last_hidden_state_cls)\n\n        return logits","metadata":{"execution":{"iopub.status.busy":"2024-10-24T19:50:54.402574Z","iopub.execute_input":"2024-10-24T19:50:54.403007Z","iopub.status.idle":"2024-10-24T19:50:54.412565Z","shell.execute_reply.started":"2024-10-24T19:50:54.40297Z","shell.execute_reply":"2024-10-24T19:50:54.410849Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Function for initializing the BERT Classifier model, optimizer, and learning rate scheduler\ndef initialize_model(epochs=4):\n    # Instantiate Bert Classifier\n    bert_classifier = Bert_Classifier(freeze_bert=False)\n\n    bert_classifier.to(device)\n\n    # Set up optimizer\n    optimizer = AdamW(bert_classifier.parameters(),\n                      lr=5e-5,    # learning rate, set to default value\n                      eps=1e-8    # decay, set to default value\n                      )\n\n    # Calculate total number of training steps\n    total_steps = len(train_dataloader) * epochs\n\n    # Define the learning rate scheduler\n    scheduler = get_linear_schedule_with_warmup(optimizer,\n                                                num_warmup_steps=0, # Default value\n                                                num_training_steps=total_steps)\n    return bert_classifier, optimizer, scheduler","metadata":{"execution":{"iopub.status.busy":"2024-10-24T19:51:00.062615Z","iopub.execute_input":"2024-10-24T19:51:00.063017Z","iopub.status.idle":"2024-10-24T19:51:00.070749Z","shell.execute_reply.started":"2024-10-24T19:51:00.062989Z","shell.execute_reply":"2024-10-24T19:51:00.069146Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"device = 'cuda' if torch.cuda.is_available() else 'cpu'\nEPOCHS=2","metadata":{"execution":{"iopub.status.busy":"2024-10-24T19:51:04.873185Z","iopub.execute_input":"2024-10-24T19:51:04.873625Z","iopub.status.idle":"2024-10-24T19:51:04.880037Z","shell.execute_reply.started":"2024-10-24T19:51:04.873592Z","shell.execute_reply":"2024-10-24T19:51:04.878509Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"bert_classifier, optimizer, scheduler = initialize_model(epochs=EPOCHS)","metadata":{"execution":{"iopub.status.busy":"2024-10-24T19:51:10.473527Z","iopub.execute_input":"2024-10-24T19:51:10.473954Z","iopub.status.idle":"2024-10-24T19:51:12.752552Z","shell.execute_reply.started":"2024-10-24T19:51:10.473919Z","shell.execute_reply":"2024-10-24T19:51:12.751433Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### BERT Training","metadata":{}},{"cell_type":"code","source":"# Define Cross entropy Loss function for the multiclass classification task\nloss_fn = nn.CrossEntropyLoss()\n\ndef bert_train(model, train_dataloader, val_dataloader=None, epochs=4, evaluation=False):\n\n    print(\"Start training...\\n\")\n    for epoch_i in range(epochs):\n        print(\"-\"*10)\n        print(\"Epoch : {}\".format(epoch_i+1))\n        print(\"-\"*10)\n        print(\"-\"*38)\n        print(f\"{'BATCH NO.':^7} | {'TRAIN LOSS':^12} | {'ELAPSED (s)':^9}\")\n        print(\"-\"*38)\n\n        # Measure the elapsed time of each epoch\n        t0_epoch, t0_batch = time.time(), time.time()\n\n        # Reset tracking variables at the beginning of each epoch\n        total_loss, batch_loss, batch_counts = 0, 0, 0\n        \n        ###TRAINING###\n\n        # Put the model into the training mode\n        model.train()\n\n        for step, batch in enumerate(train_dataloader):\n            batch_counts +=1\n            \n            b_input_ids, b_attn_mask, b_labels = tuple(t.to(device) for t in batch)\n\n            # Zero out any previously calculated gradients\n            model.zero_grad()\n\n            # Perform a forward pass and get logits.\n            logits = model(b_input_ids, b_attn_mask)\n\n            # Compute loss and accumulate the loss values\n            loss = loss_fn(logits, b_labels)\n            batch_loss += loss.item()\n            total_loss += loss.item()\n\n            # Perform a backward pass to calculate gradients\n            loss.backward()\n\n            # Clip the norm of the gradients to 1.0 to prevent \"exploding gradients\"\n            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n\n            # Update model parameters:\n            # fine tune BERT params and train additional dense layers\n            optimizer.step()\n            # update learning rate\n            scheduler.step()\n\n            # Print the loss values and time elapsed for every 100 batches\n            if (step % 100 == 0 and step != 0) or (step == len(train_dataloader) - 1):\n                # Calculate time elapsed for 20 batches\n                time_elapsed = time.time() - t0_batch\n                \n                print(f\"{step:^9} | {batch_loss / batch_counts:^12.6f} | {time_elapsed:^9.2f}\")\n\n                # Reset batch tracking variables\n                batch_loss, batch_counts = 0, 0\n                t0_batch = time.time()\n\n        # Calculate the average loss over the entire training data\n        avg_train_loss = total_loss / len(train_dataloader)\n\n        ###EVALUATION###\n        \n        # Put the model into the evaluation mode\n        model.eval()\n        \n        # Define empty lists to host accuracy and validation for each batch\n        val_accuracy = []\n        val_loss = []\n\n        for batch in val_dataloader:\n            batch_input_ids, batch_attention_mask, batch_labels = tuple(t.to(device) for t in batch)\n            \n            # We do not want to update the params during the evaluation,\n            # So we specify that we dont want to compute the gradients of the tensors\n            # by calling the torch.no_grad() method\n            with torch.no_grad():\n                logits = model(batch_input_ids, batch_attention_mask)\n\n            loss = loss_fn(logits, batch_labels)\n\n            val_loss.append(loss.item())\n\n            # Get the predictions starting from the logits (get index of highest logit)\n            preds = torch.argmax(logits, dim=1).flatten()\n\n            # Calculate the validation accuracy \n            accuracy = (preds == batch_labels).cpu().numpy().mean() * 100\n            val_accuracy.append(accuracy)\n\n        # Compute the average accuracy and loss over the validation set\n        val_loss = np.mean(val_loss)\n        val_accuracy = np.mean(val_accuracy)\n        \n        # Print performance over the entire training data\n        time_elapsed = time.time() - t0_epoch\n        print(\"-\"*61)\n        print(f\"{'AVG TRAIN LOSS':^12} | {'VAL LOSS':^10} | {'VAL ACCURACY (%)':^9} | {'ELAPSED (s)':^9}\")\n        print(\"-\"*61)\n        print(f\"{avg_train_loss:^14.6f} | {val_loss:^10.6f} | {val_accuracy:^17.2f} | {time_elapsed:^9.2f}\")\n        print(\"-\"*61)\n        print(\"\\n\")\n    \n    print(\"Training complete!\")","metadata":{"execution":{"iopub.status.busy":"2024-10-24T19:51:23.702312Z","iopub.execute_input":"2024-10-24T19:51:23.70272Z","iopub.status.idle":"2024-10-24T19:51:23.721151Z","shell.execute_reply.started":"2024-10-24T19:51:23.702687Z","shell.execute_reply":"2024-10-24T19:51:23.71933Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"start_time = time.time()\n\n\nbert_train(bert_classifier, train_dataloader, val_dataloader, epochs=EPOCHS)\n\nend_time = time.time()\nruntime['Bert'] = end_time - start_time","metadata":{"execution":{"iopub.status.busy":"2024-10-24T19:51:32.183817Z","iopub.execute_input":"2024-10-24T19:51:32.184268Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{},"outputs":[],"execution_count":null}]}